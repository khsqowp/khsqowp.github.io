--- 
title: "AWS ë°ì´í„° ë¶„ì„ê³¼ ì‹œìŠ¤í…œ ê´€ë¦¬: Redshiftë¶€í„° SageMakerê¹Œì§€"
date: 2025-12-11
excerpt: "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Amazon Redshiftì™€ ê¸°ê³„í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì„ ìœ„í•œ SageMaker ë“± AWSì˜ ë°ì´í„° ë¶„ì„ ë° ML ì„œë¹„ìŠ¤ë¥¼ ì‹¬ì¸µ í•™ìŠµí•©ë‹ˆë‹¤. ë˜í•œ CloudWatchì™€ EventBridgeë¥¼ í™œìš©í•œ ëª¨ë‹ˆí„°ë§ ìë™í™” ë° ì‹œìŠ¤í…œ ìš´ì˜ ê´€ë¦¬ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤."
categories:
  - AWS-Cloud
tags:
  - AWS-Cloud
  - SK_Rookies
---

# ğŸ“ AWS í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ 7ì¼ì°¨ ê°•ì˜ ë…¸íŠ¸

**ê°•ì˜ì¼**: 2025ë…„ 12ì›” 11ì¼
**ì£¼ì œ**: ë°ì´í„° ë¶„ì„ ë° ì‹œìŠ¤í…œ ê´€ë¦¬ ì„œë¹„ìŠ¤
**ê³¼ì •**: AWS í´ë¼ìš°ë“œ ì‹¤ë¬´ êµìœ¡ (7ì¼ì°¨ - ìµœì¢…ì¼)

---

## ğŸ“š ê°•ì˜ ê°œìš”

### ğŸ¯ í•™ìŠµ ëª©í‘œ

7ì¼ ë™ì•ˆ ì§„í–‰ëœ AWS í´ë¼ìš°ë“œ êµìœ¡ ê³¼ì •ì˜ ë§ˆì§€ë§‰ ë‚ ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ë‹¤ìŒ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤:

1. **ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤** - ë¹…ë°ì´í„° ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ ë¶„ì„
2. **ê¸°ê³„í•™ìŠµ ì„œë¹„ìŠ¤** - AWS SageMakerë¥¼ í™œìš©í•œ ML íŒŒì´í”„ë¼ì¸
3. **ì‹œìŠ¤í…œ ê´€ë¦¬ ì„œë¹„ìŠ¤** - Infrastructure as Code, ëª¨ë‹ˆí„°ë§, ìë™í™”
4. **ê°œë°œ ì§€ì› ì„œë¹„ìŠ¤** - CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì„±
5. **ì‹¤ìŠµ** - CloudWatch ê²½ë³´, EventBridge ì„¤ì •
6. **ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸** - í†µí•© ì›¹ ì„œë¹„ìŠ¤ ì¸í”„ë¼ êµ¬ì¶•

ğŸ’¡ **ì¤‘ìš”!**: ì˜¤ëŠ˜ ë°°ìš°ëŠ” ë‚´ìš©ë“¤ì€ ì‹¤ë¬´ì—ì„œ ë§¤ìš° ìì£¼ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ì„œë¹„ìŠ¤ë“¤ì…ë‹ˆë‹¤. íŠ¹íˆ ì‹œìŠ¤í…œ ê´€ë¦¬ ì„œë¹„ìŠ¤ëŠ” AWS ìš´ì˜ì˜ ê¸°ë³¸ì´ ë˜ëŠ” ì„œë¹„ìŠ¤ë“¤ì´ë¯€ë¡œ ì§‘ì¤‘ì ìœ¼ë¡œ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.

---

## ğŸ”„ ì§€ë‚œ ì‹œê°„ ë³µìŠµ (6ì¼ì°¨ ë‚´ìš© ìš”ì•½)

ì§€ë‚œ ì‹œê°„ì—ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ë° ë³´ì•ˆ ì„œë¹„ìŠ¤ë¥¼ ì¤‘ì ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤

#### 1ï¸âƒ£ Amazon Redshift - ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤

**ê°œë… ë° íŠ¹ì§•:**

Amazon RedshiftëŠ” **ì»¬ëŸ¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€(Columnar Storage)**ë¥¼ ì§€ì›í•˜ëŠ” ê´€ê³„í˜• ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    A[ì™¸ë¶€ ìš”ì²­] --> B[ë¦¬ë” ë…¸ë“œ Leader Node]
    B --> C[ì»´í“¨íŒ… ë…¸ë“œ 1]
    B --> D[ì»´í“¨íŒ… ë…¸ë“œ 2]
    B --> E[ì»´í“¨íŒ… ë…¸ë“œ N]
    C --> F[ë¡œì»¬ ìŠ¤í† ë¦¬ì§€]
    D --> G[ë¡œì»¬ ìŠ¤í† ë¦¬ì§€]
    E --> H[ë¡œì»¬ ìŠ¤í† ë¦¬ì§€]
    C -.-> I[S3 í†µí•©]
    D -.-> I
    E -.-> I

    style B fill:#ff9999
    style C fill:#99ccff
    style D fill:#99ccff
    style E fill:#99ccff
    style I fill:#99ff99
```

**êµ¬ì„± ìš”ì†Œ:**

| êµ¬ë¶„ | ì—­í•  | íŠ¹ì§• |
|------|------|------|
| **ë¦¬ë” ë…¸ë“œ** | ì™¸ë¶€ ìš”ì²­ ìˆ˜ì‹  ë° ì¿¼ë¦¬ ë¶„ì‚° | - SQL ì¿¼ë¦¬ íŒŒì‹±<br>- ì‹¤í–‰ ê³„íš ìˆ˜ë¦½<br>- ê²°ê³¼ ì§‘ê³„ |
| **ì»´í“¨íŒ… ë…¸ë“œ** | ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ | - ë°ì´í„° ì €ì¥<br>- ì¿¼ë¦¬ ì‹¤í–‰<br>- ë³‘ë ¬ ì²˜ë¦¬ |
| **ë¡œì»¬ ìŠ¤í† ë¦¬ì§€** | ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ì €ì¥ | - SSD ê¸°ë°˜<br>- ê³ ì† ì•¡ì„¸ìŠ¤ |
| **S3 í†µí•©** | ì¥ê¸° ë°ì´í„° ì €ì¥ | - ë¹„ìš© ì ˆê°<br>- ë¬´ì œí•œ í™•ì¥ |

**ì‹¤í–‰ íë¦„:**

1. í´ë¼ì´ì–¸íŠ¸ê°€ SQL ì¿¼ë¦¬ë¥¼ ë¦¬ë” ë…¸ë“œì— ì „ì†¡
2. ë¦¬ë” ë…¸ë“œê°€ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
3. ì‹¤í–‰ ê³„íšì„ ê° ì»´í“¨íŒ… ë…¸ë“œì— ë¶„ì‚°
4. ì»´í“¨íŒ… ë…¸ë“œê°€ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ë˜ëŠ” S3ì—ì„œ ë°ì´í„° ì½ê¸°
5. ê° ë…¸ë“œì—ì„œ ë³‘ë ¬ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
6. ê²°ê³¼ë¥¼ ë¦¬ë” ë…¸ë“œë¡œ ì „ì†¡
7. ë¦¬ë” ë…¸ë“œê°€ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜

ğŸ“Œ **ë…¸íŠ¸**: RedshiftëŠ” OLAP(Online Analytical Processing) ì›Œí¬ë¡œë“œì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

---

#### 2ï¸âƒ£ AWS DMS (Database Migration Service) - ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜

**ì£¼ìš” ê¸°ëŠ¥:**

```mermaid
flowchart LR
    A[ì˜¨í”„ë ˆë¯¸ìŠ¤ DB] -->|DMS| B[AWS RDS]
    C[AWS RDS] -->|DMS| D[ë‹¤ë¥¸ AWS DB]
    E[ë™ì¼ ì—”ì§„] -.->|ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜| F[ì™„ë£Œ]
    G[ì´ê¸°ì¢… ì—”ì§„] -.->|SCT í•„ìš”| H[ìˆ˜ë™ ì‘ì—… í•„ìš”]

    style A fill:#ffcccc
    style B fill:#ccffcc
    style C fill:#ccffcc
    style D fill:#ccffcc
    style E fill:#ffffcc
    style G fill:#ffccff
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ìœ í˜•:**

| ìœ í˜• | ì„¤ëª… | íŠ¹ì§• |
|------|------|------|
| **ë™ì¼ ì—”ì§„** | MySQL â†’ RDS MySQL<br>Oracle â†’ RDS Oracle | - ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜<br>- ìŠ¤í‚¤ë§ˆ ë³€í™˜ ë¶ˆí•„ìš”<br>- ë¹ ë¥¸ ë§ˆì´ê·¸ë ˆì´ì…˜ |
| **ì´ê¸°ì¢… ì—”ì§„** | Oracle â†’ Aurora PostgreSQL<br>SQL Server â†’ MySQL | - SCT(Schema Conversion Tool) í•„ìš”<br>- ìˆ˜ë™ ê²€ì¦ í•„ìš”<br>- í˜¸í™˜ì„± ê²€í†  |

**ë§ˆì´ê·¸ë ˆì´ì…˜ ëª¨ë“œ:**

1. **ì¼ê´„ ë§ˆì´ê·¸ë ˆì´ì…˜ (Full Load)**
   - í•œ ë²ˆì— ëª¨ë“  ë°ì´í„° ì´ê´€
   - ë‹¤ìš´íƒ€ì„ ë°œìƒ
   - ì†Œê·œëª¨ ë°ì´í„°ë² ì´ìŠ¤ì— ì í•©

2. **ì§€ì†ì  ë§ˆì´ê·¸ë ˆì´ì…˜ (CDC - Change Data Capture)**
   - ì´ˆê¸° ì „ì²´ ë³µì œ í›„ ë³€ê²½ë¶„ë§Œ ì§€ì† ë™ê¸°í™”
   - ë‹¤ìš´íƒ€ì„ ìµœì†Œí™”
   - ì‹¤ì‹œê°„ ë™ê¸°í™”
   - ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©

**ì‹¤ìŠµ ì‹œë‚˜ë¦¬ì˜¤:**

```bash
# DMS íƒœìŠ¤í¬ ìƒì„± ì˜ˆì‹œ (AWS CLI)
aws dms create-replication-task \
    --replication-task-identifier my-migration-task \
    --source-endpoint-arn arn:aws:dms:region:account:endpoint:source \
    --target-endpoint-arn arn:aws:dms:region:account:endpoint:target \
    --replication-instance-arn arn:aws:dms:region:account:rep:instance \
    --migration-type full-load-and-cdc \
    --table-mappings file://table-mappings.json
```

---

#### 3ï¸âƒ£ AWS SCT (Schema Conversion Tool)

**ì—­í• :**

ì´ê¸°ì¢… ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ìŠ¤í‚¤ë§ˆ ë³€í™˜ì„ ì§€ì›í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

**ì£¼ìš” ê¸°ëŠ¥:**

1. **ìë™ ìŠ¤í‚¤ë§ˆ ë³€í™˜**
   - í…Œì´ë¸”, ë·°, ì¸ë±ìŠ¤ ë³€í™˜
   - ì €ì¥ í”„ë¡œì‹œì € ë³€í™˜
   - íŠ¸ë¦¬ê±° ë³€í™˜

2. **í˜¸í™˜ì„± ë¶„ì„**
   - ë³€í™˜ ê°€ëŠ¥ í•­ëª© ì‹ë³„
   - ìˆ˜ë™ ì‘ì—… í•„ìš” í•­ëª© í‘œì‹œ
   - ë³€í™˜ ê°€ì´ë“œ ì œê³µ

3. **ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ**
   - ë³€í™˜ ì„±ê³µë¥ 
   - ì ì¬ì  ë¬¸ì œì 
   - ê¶Œì¥ ì‚¬í•­

âš ï¸ **ì£¼ì˜**: SCTëŠ” 100% ìë™ ë³€í™˜ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³µì¡í•œ SQL ë¬¸, íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥ì€ ìˆ˜ë™ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

---

#### 4ï¸âƒ£ Amazon ElastiCache - ì¸ë©”ëª¨ë¦¬ ë°ì´í„° ì €ì¥ì†Œ

**ì§€ì› ì—”ì§„:**

| ì—”ì§„ | íŠ¹ì§• | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|-----------|
| **Memcached** | - ë‹¨ìˆœí•œ key-value ìŠ¤í† ì–´<br>- ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì› | - DB ìºì‹œ<br>- ì„¸ì…˜ ì €ì¥ì†Œ |
| **Redis** | - ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì§€ì›<br>- ë³µì œ ë° ì§€ì†ì„± ì§€ì›<br>- Pub/Sub ê¸°ëŠ¥ | - DB ìºì‹œ<br>- ì„¸ì…˜ ì €ì¥ì†Œ<br>- ê²Œì„ ë¦¬ë”ë³´ë“œ<br>- ì‹¤ì‹œê°„ ë¶„ì„ |

**Redis ë°ì´í„° êµ¬ì¡°:**

```mermaid
graph TD
    A[Redis ë°ì´í„° êµ¬ì¡°] --> B[String]
    A --> C[List]
    A --> D[Set]
    A --> E[Sorted Set]
    A --> F[Hash]
    A --> G[Bitmap]
    A --> H[HyperLogLog]

    B --> B1[ê°„ë‹¨í•œ key-value]
    C --> C1[ìˆœì„œìˆëŠ” ë¦¬ìŠ¤íŠ¸]
    D --> D1[ì¤‘ë³µì—†ëŠ” ì§‘í•©]
    E --> E1[ì •ë ¬ëœ ì§‘í•©]
    F --> F1[í•„ë“œ-ê°’ ìŒ]
```

**í™œìš© íŒ¨í„´:**

1. **ë°ì´í„°ë² ì´ìŠ¤ ìºì‹œ**
```python
# Redisë¥¼ DB ìºì‹œë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ
import redis
import mysql.connector

redis_client = redis.Redis(host='elasticache-endpoint', port=6379)
db = mysql.connector.connect(user='user', password='pass', database='mydb')

def get_user(user_id):
    # 1. Redis ìºì‹œ í™•ì¸
    cached = redis_client.get(f"user:{user_id}")
    if cached:
        return json.loads(cached)

    # 2. ìºì‹œ ë¯¸ìŠ¤ - DB ì¡°íšŒ
    cursor = db.cursor()
    cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
    user = cursor.fetchone()

    # 3. Redisì— ì €ì¥ (TTL 3600ì´ˆ)
    redis_client.setex(f"user:{user_id}", 3600, json.dumps(user))

    return user
```

2. **ì„¸ì…˜ ìŠ¤í† ì–´**
```python
# Flask ì„¸ì…˜ì„ Redisì— ì €ì¥
from flask import Flask, session
from flask_session import Session

app = Flask(__name__)
app.config['SESSION_TYPE'] = 'redis'
app.config['SESSION_REDIS'] = redis.from_url('redis://elasticache-endpoint:6379')
Session(app)

@app.route('/login', methods=['POST'])
def login():
    session['user_id'] = request.form['user_id']
    return 'Logged in'
```

3. **ê²Œì„ ë¦¬ë”ë³´ë“œ**
```python
# Redis Sorted Setì„ ì´ìš©í•œ ë¦¬ë”ë³´ë“œ
def add_score(user_id, score):
    redis_client.zadd('leaderboard', {user_id: score})

def get_top_10():
    return redis_client.zrevrange('leaderboard', 0, 9, withscores=True)

def get_rank(user_id):
    rank = redis_client.zrevrank('leaderboard', user_id)
    return rank + 1 if rank is not None else None
```

ğŸ“Œ **ë…¸íŠ¸**: ElastiCacheëŠ” ì½ê¸° ì§‘ì•½ì ì¸ ì›Œí¬ë¡œë“œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìì£¼ ì¡°íšŒë˜ì§€ë§Œ ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.

---

#### 5ï¸âƒ£ ê¸°íƒ€ NoSQL ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤

**1. Amazon MemoryDB for Redis**
- Redis í˜¸í™˜ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤
- ë§ˆì´í¬ë¡œì´ˆ ì½ê¸° ì§€ì—° ì‹œê°„
- ì˜êµ¬ ì§€ì†ì„± ì œê³µ (ElastiCacheì™€ì˜ ì°¨ì´ì )

**2. Amazon DocumentDB**
- MongoDB í˜¸í™˜ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤
- JSON í˜•ì‹ ë°ì´í„° ì €ì¥
- MongoDB API í˜¸í™˜

**3. Amazon Neptune**
- ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤
- ì†Œì…œ ë„¤íŠ¸ì›Œí¬, ì¶”ì²œ ì‹œìŠ¤í…œì— í™œìš©
- Gremlin ë° SPARQL ì¿¼ë¦¬ ì–¸ì–´ ì§€ì›

**4. Amazon Quantum Ledger Database (QLDB)**
- ì›ì¥ ë°ì´í„°ë² ì´ìŠ¤
- ë³€ê²½ ë¶ˆê°€ëŠ¥í•œ íŠ¸ëœì­ì…˜ ë¡œê·¸
- ì•”í˜¸í•™ì ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥í•œ íŠ¸ëœì­ì…˜ ë¡œê·¸

**5. Amazon Timestream**
- ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤
- IoT ë° ìš´ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ìµœì í™”
- ìë™ ë°ì´í„° ê³„ì¸µí™” ë° ì••ì¶•

**6. Amazon Managed Blockchain**
- ë¸”ë¡ì²´ì¸ ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬ ì„œë¹„ìŠ¤
- Hyperledger Fabric ë° Ethereum ì§€ì›

---

### ğŸ” ë³´ì•ˆ ì„œë¹„ìŠ¤ ë³µìŠµ

#### 1ï¸âƒ£ ë³´ì•ˆì˜ 3ëŒ€ ìš”ì†Œ (CIA Triad)

```mermaid
graph TD
    A[ì •ë³´ ë³´ì•ˆ] --> B[ê¸°ë°€ì„± Confidentiality]
    A --> C[ë¬´ê²°ì„± Integrity]
    A --> D[ê°€ìš©ì„± Availability]

    B --> B1[ì¸ê°€ëœ ì‚¬ìš©ìë§Œ ì ‘ê·¼]
    C --> C1[ì†ìƒë˜ì§€ ì•Šì€ ì •ë³´]
    D --> D1[í•„ìš”í•  ë•Œ ì‚¬ìš© ê°€ëŠ¥]

    style A fill:#ff9999
    style B fill:#99ccff
    style C fill:#99ff99
    style D fill:#ffff99
```

**ì •ì˜:**

| ìš”ì†Œ | ì„¤ëª… | ë³´ì•ˆ í†µì œ |
|------|------|-----------|
| **ê¸°ë°€ì„±<br>(Confidentiality)** | ì¸ê°€ëœ ì‚¬ìš©ìë§Œ ì •ë³´ì— ì ‘ê·¼ ê°€ëŠ¥ | - ì•”í˜¸í™”<br>- ì ‘ê·¼ ì œì–´<br>- ì¸ì¦ |
| **ë¬´ê²°ì„±<br>(Integrity)** | ì •ë³´ê°€ ë¬´ë‹¨ìœ¼ë¡œ ë³€ê²½ë˜ì§€ ì•ŠìŒ | - í•´ì‹œ<br>- ë””ì§€í„¸ ì„œëª…<br>- ë²„ì „ ê´€ë¦¬ |
| **ê°€ìš©ì„±<br>(Availability)** | í•„ìš”í•  ë•Œ ì •ë³´ë¥¼ ì‚¬ìš© ê°€ëŠ¥ | - ë°±ì—…<br>- ì´ì¤‘í™”<br>- ì¬í•´ ë³µêµ¬ |

**í†µí•© ì •ì˜:**
> "ì¸ê°€ëœ ì‚¬ìš©ìë§Œì´ ì†ìƒë˜ì§€ ì•Šì€ ìƒíƒœì˜ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤."

---

#### 2ï¸âƒ£ AWS ê³µë™ ì±…ì„ ëª¨ë¸ (Shared Responsibility Model)

```mermaid
graph TD
    A[AWS ì±…ì„] --> A1[í´ë¼ìš°ë“œì˜ ë³´ì•ˆ<br>Security OF the Cloud]
    B[ê³ ê° ì±…ì„] --> B1[í´ë¼ìš°ë“œ ë‚´ë¶€ì˜ ë³´ì•ˆ<br>Security IN the Cloud]

    A1 --> A2[ë¬¼ë¦¬ì  ì¸í”„ë¼]
    A1 --> A3[ë„¤íŠ¸ì›Œí¬ ì¸í”„ë¼]
    A1 --> A4[í•˜ë“œì›¨ì–´]
    A1 --> A5[ì†Œí”„íŠ¸ì›¨ì–´ - ê°€ìƒí™” ë ˆì´ì–´]

    B1 --> B2[ê²ŒìŠ¤íŠ¸ OS]
    B1 --> B3[ì• í”Œë¦¬ì¼€ì´ì…˜]
    B1 --> B4[ë°ì´í„°]
    B1 --> B5[ë°©í™”ë²½ ê·œì¹™]
    B1 --> B6[ì•”í˜¸í™”]

    style A fill:#ff9999
    style B fill:#99ccff
```

**ì„œë¹„ìŠ¤ë³„ ì±…ì„ ë²”ìœ„:**

| ì„œë¹„ìŠ¤ ìœ í˜• | AWS ì±…ì„ | ê³ ê° ì±…ì„ |
|------------|----------|-----------|
| **IaaS<br>(EC2)** | - í•˜ë“œì›¨ì–´<br>- ë„¤íŠ¸ì›Œí¬<br>- í•˜ì´í¼ë°”ì´ì € | - OS íŒ¨ì¹˜<br>- ì• í”Œë¦¬ì¼€ì´ì…˜<br>- ë°ì´í„°<br>- ë³´ì•ˆê·¸ë£¹ |
| **PaaS<br>(RDS)** | - í•˜ë“œì›¨ì–´<br>- ë„¤íŠ¸ì›Œí¬<br>- í•˜ì´í¼ë°”ì´ì €<br>- OS íŒ¨ì¹˜ | - DB ì‚¬ìš©ì ê´€ë¦¬<br>- ë„¤íŠ¸ì›Œí¬ ì„¤ì •<br>- ë°ì´í„° |
| **SaaS<br>(S3)** | - í•˜ë“œì›¨ì–´<br>- ë„¤íŠ¸ì›Œí¬<br>- ì†Œí”„íŠ¸ì›¨ì–´<br>- ìŠ¤í† ë¦¬ì§€ | - ë°ì´í„°<br>- ì•¡ì„¸ìŠ¤ ì •ì±…<br>- ì•”í˜¸í™” ì„¤ì • |

ğŸ’¡ **ì¤‘ìš”!**: ê³µë™ ì±…ì„ ëª¨ë¸ì„ ì´í•´í•˜ëŠ” ê²ƒì€ AWS ë³´ì•ˆì˜ ì²« ê±¸ìŒì…ë‹ˆë‹¤. ê³ ê°ì€ ìì‹ ì˜ ì±…ì„ ì˜ì—­ì„ ëª…í™•íˆ ì¸ì§€í•˜ê³  ì ì ˆí•œ ë³´ì•ˆ í†µì œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

---

#### 3ï¸âƒ£ AWS CloudTrail - ê°ì‚¬ ë¡œê¹…

**í•µì‹¬ ê¸°ëŠ¥:**

CloudTrailì€ AWS ê³„ì •ì˜ **ëª¨ë“  API í˜¸ì¶œì„ ê¸°ë¡**í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ë¡œê¹… ëŒ€ìƒ:**

- ëˆ„ê°€ (User/Role)
- ì–¸ì œ (Timestamp)
- ì–´ë””ì„œ (Source IP)
- ë¬´ì—‡ì„ (API Action)
- ì–´ëŠ ë¦¬ì†ŒìŠ¤ì— (Resource ARN)
- ê²°ê³¼ (Success/Failure)

**Trail êµ¬ì„± ìš”ì†Œ:**

```mermaid
sequenceDiagram
    participant U as ì‚¬ìš©ì
    participant AWS as AWS ì„œë¹„ìŠ¤
    participant CT as CloudTrail
    participant S3 as S3 ë²„í‚·
    participant CW as CloudWatch Logs

    U->>AWS: API í˜¸ì¶œ
    AWS->>CT: ì´ë²¤íŠ¸ ê¸°ë¡
    CT->>S3: ë¡œê·¸ íŒŒì¼ ì €ì¥
    CT->>CW: ì‹¤ì‹œê°„ ë¡œê·¸ ì „ì†¡

    Note over S3: ì¥ê¸° ë³´ê´€ (90ì¼ ì´ìƒ)
    Note over CW: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```

**ë¡œê·¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦:**

CloudTrailì€ ë¡œê·¸ íŒŒì¼ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ **ë‹¤ì´ì œìŠ¤íŠ¸ íŒŒì¼(Digest File)**ì„ ì œê³µí•©ë‹ˆë‹¤.

```bash
# ë¡œê·¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ëª…ë ¹ì–´
aws cloudtrail validate-logs \
    --trail-arn arn:aws:cloudtrail:region:account:trail/name \
    --start-time 2025-12-11T00:00:00Z \
    --end-time 2025-12-11T23:59:59Z
```

**ë‹¤ì´ì œìŠ¤íŠ¸ íŒŒì¼ êµ¬ì¡°:**

```json
{
  "digestStartTime": "2025-12-11T00:00:00Z",
  "digestEndTime": "2025-12-11T01:00:00Z",
  "digestS3Bucket": "my-cloudtrail-bucket",
  "logFiles": [
    {
      "s3Object": "AWSLogs/123456789012/CloudTrail/us-east-1/2025/12/11/...",
      "hashValue": "4f8c9b3a2e1d...",
      "hashAlgorithm": "SHA-256"
    }
  ]
}
```

ğŸ“Œ **ë…¸íŠ¸**: CloudTrail ë¡œê·¸ëŠ” ê·œì • ì¤€ìˆ˜, ë³´ì•ˆ ë¶„ì„, ìš´ì˜ ë¬¸ì œ í•´ê²°ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ëª¨ë“  AWS ê³„ì •ì—ì„œ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤.

---

#### 4ï¸âƒ£ VPC Flow Logs - ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ë¡œê¹…

**ê°œë…:**

VPC ë‚´ì˜ **ENI(Elastic Network Interface)**ì—ì„œ ì†¡ìˆ˜ì‹ ë˜ëŠ” **IP íŠ¸ë˜í”½ ì •ë³´**ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.

**ë¡œê·¸ í˜•ì‹:**

```
version account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status
```

**ì‹¤ì œ ë¡œê·¸ ì˜ˆì‹œ:**

```
2 123456789012 eni-1234abcd 203.0.113.12 10.0.0.15 49152 443 6 10 4096 1639234567 1639234627 ACCEPT OK
2 123456789012 eni-1234abcd 10.0.0.15 198.51.100.45 443 49153 6 5 2048 1639234567 1639234627 REJECT OK
```

**í•„ë“œ ì„¤ëª…:**

| í•„ë“œ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| version | ë¡œê·¸ ë²„ì „ | 2 |
| account-id | AWS ê³„ì • ID | 123456789012 |
| interface-id | ENI ID | eni-1234abcd |
| srcaddr | ì†ŒìŠ¤ IP | 203.0.113.12 |
| dstaddr | ëª©ì ì§€ IP | 10.0.0.15 |
| srcport | ì†ŒìŠ¤ í¬íŠ¸ | 49152 |
| dstport | ëª©ì ì§€ í¬íŠ¸ | 443 |
| protocol | í”„ë¡œí† ì½œ ë²ˆí˜¸ | 6 (TCP) |
| packets | íŒ¨í‚· ìˆ˜ | 10 |
| bytes | ë°”ì´íŠ¸ ìˆ˜ | 4096 |
| start | ì‹œì‘ ì‹œê°„ (Unix) | 1639234567 |
| end | ì¢…ë£Œ ì‹œê°„ (Unix) | 1639234627 |
| action | ACCEPT / REJECT | ACCEPT |
| log-status | OK / NODATA / SKIPDATA | OK |

**í™œìš© ì‚¬ë¡€:**

1. **ë³´ì•ˆ ë¶„ì„**
   - ë¹„ì •ìƒì ì¸ íŠ¸ë˜í”½ íŒ¨í„´ íƒì§€
   - ë¬´ë‹¨ ì ‘ê·¼ ì‹œë„ ì‹ë³„

2. **ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°**
   - ì—°ê²° ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
   - ì„±ëŠ¥ ë³‘ëª© ì§€ì  íŒŒì•…

3. **ê·œì • ì¤€ìˆ˜**
   - ë„¤íŠ¸ì›Œí¬ í™œë™ ê°ì‚¬
   - ì»´í”Œë¼ì´ì–¸ìŠ¤ ë³´ê³ ì„œ ìƒì„±

âš ï¸ **ì£¼ì˜**: VPC Flow LogsëŠ” Wiresharkì™€ ê°™ì€ íŒ¨í‚· ìº¡ì²˜ ë„êµ¬ê°€ ì•„ë‹™ë‹ˆë‹¤. **íŒ¨í‚· ë‚´ìš©ì€ ê¸°ë¡í•˜ì§€ ì•Šìœ¼ë©°**, ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡í•©ë‹ˆë‹¤.

---

ì´ì œ ì„¹ì…˜ 2ë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.
## ğŸ“Š ë°ì´í„° ë¶„ì„ì˜ ì´í•´

### ğŸ¯ ë°ì´í„° ë¶„ì„ì´ë€?

ë°ì´í„° ë¶„ì„ì€ **ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ì²˜ë¦¬**í•˜ì—¬ **ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ëŠ” ì¸ì‚¬ì´íŠ¸**ë¥¼ ë„ì¶œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

**ëª©ì :**
- ê²½ì˜ ì˜ì‚¬ê²°ì • ì§€ì›
- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë°œê²¬
- ë°ì´í„° ê¸°ë°˜ ì „ëµ ìˆ˜ë¦½
- ìš´ì˜ ìµœì í™”

```mermaid
graph LR
    A[ì›ì‹œ ë°ì´í„°<br>Raw Data] --> B[ë°ì´í„° ìˆ˜ì§‘<br>Collection]
    B --> C[ë°ì´í„° ì €ì¥<br>Storage]
    C --> D[ë°ì´í„° ì²˜ë¦¬<br>Processing]
    D --> E[ë°ì´í„° ë¶„ì„<br>Analysis]
    E --> F[ì‹œê°í™”<br>Visualization]
    F --> G[ì˜ì‚¬ê²°ì •<br>Decision]

    style A fill:#ffcccc
    style C fill:#ccffcc
    style E fill:#ccccff
    style G fill:#ffffcc
```

---

### ğŸ“ˆ ë°ì´í„° ë¶„ì„ vs ê¸°ê³„í•™ìŠµ

| êµ¬ë¶„ | ë°ì´í„° ë¶„ì„ | ê¸°ê³„í•™ìŠµ |
|------|------------|----------|
| **ëª©ì ** | ê³¼ê±° ë°ì´í„°ì—ì„œ íŒ¨í„´ ë°œê²¬ | ë¯¸ë˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• |
| **ë°©ë²•** | í†µê³„ ë¶„ì„, ì‹œê°í™” | ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ |
| **ê²°ê³¼** | ì¸ì‚¬ì´íŠ¸, ë¦¬í¬íŠ¸ | ì˜ˆì¸¡ ëª¨ë¸ |
| **ì‹œê°„ ê´€ì ** | ê³¼ê±° â†’ í˜„ì¬ | ê³¼ê±° â†’ ë¯¸ë˜ |
| **ì˜ˆì‹œ** | "ì§€ë‚œ ë¶„ê¸° ë§¤ì¶œì´ 20% ì¦ê°€í–ˆë‹¤" | "ë‹¤ìŒ ë¶„ê¸° ë§¤ì¶œì€ 15% ì¦ê°€í•  ê²ƒì´ë‹¤" |

**ë°ì´í„° ë¶„ì„ í”„ë¡œì„¸ìŠ¤:**

```mermaid
flowchart TD
    A[ë°ì´í„° ìˆ˜ì§‘] --> B{ë°ì´í„° í’ˆì§ˆ ê²€ì¦}
    B -->|OK| C[ë°ì´í„° ì •ì œ]
    B -->|NG| A
    C --> D[ë°ì´í„° ë³€í™˜]
    D --> E[ë°ì´í„° ë¶„ì„]
    E --> F[ì¸ì‚¬ì´íŠ¸ ë„ì¶œ]
    F --> G[ì‹œê°í™” ë° ë³´ê³ ì„œ]
    G --> H[ì˜ì‚¬ê²°ì •]

    style A fill:#e1f5ff
    style C fill:#fff4e1
    style E fill:#e8f5e9
    style H fill:#fce4ec
```

**ê¸°ê³„í•™ìŠµ í”„ë¡œì„¸ìŠ¤:**

```mermaid
flowchart TD
    A[ë°ì´í„° ìˆ˜ì§‘] --> B[ë°ì´í„° ì „ì²˜ë¦¬]
    B --> C[íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§]
    C --> D[ë°ì´í„° ë¶„í• <br>Train/Validation/Test]
    D --> E[ëª¨ë¸ í•™ìŠµ]
    E --> F[ëª¨ë¸ í‰ê°€]
    F -->|ì„±ëŠ¥ ë¶€ì¡±| G[í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹]
    G --> E
    F -->|ì„±ëŠ¥ ì¶©ë¶„| H[ëª¨ë¸ ë°°í¬]
    H --> I[ì˜ˆì¸¡ ì„œë¹„ìŠ¤]

    style A fill:#e1f5ff
    style E fill:#e8f5e9
    style H fill:#fce4ec
    style I fill:#fff9c4
```

ğŸ’¡ **ì¤‘ìš”!**: ë°ì´í„° ë¶„ì„ì€ "ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ê°€"ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ê³ , ê¸°ê³„í•™ìŠµì€ "ì•ìœ¼ë¡œ ë¬´ì—‡ì´ ì¼ì–´ë‚  ê²ƒì¸ê°€"ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

### ğŸ”„ ë¶„ì„ ìœ í˜•

#### 1ï¸âƒ£ ë°°ì¹˜ ë¶„ì„ (Batch Analysis)

**íŠ¹ì§•:**
- ì €ì¥ëœ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
- ë³µì¡í•œ ì—°ì‚° ê°€ëŠ¥
- ê²°ê³¼ ìƒì„±ê¹Œì§€ ì‹œê°„ ì†Œìš” (ë¶„ ~ ì‹œê°„ ë‹¨ìœ„)

**í™œìš© ì‚¬ë¡€:**
- ì¼ì¼ ë§¤ì¶œ ë¦¬í¬íŠ¸
- ì›”ê°„ ê³ ê° ë¶„ì„
- ë¶„ê¸°ë³„ ì¬ë¬´ ë¶„ì„

**í”„ë¡œì„¸ìŠ¤:**

```mermaid
sequenceDiagram
    participant DS as ë°ì´í„° ì†ŒìŠ¤
    participant DL as ë°ì´í„° ë ˆì´í¬
    participant BA as ë°°ì¹˜ ë¶„ì„ ì‘ì—…
    participant DW as ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤
    participant BI as BI ë„êµ¬

    DS->>DL: ë°ì´í„° ìˆ˜ì§‘<br>(ë§¤ì‹œê°„/ë§¤ì¼)
    Note over DL: ì›ì‹œ ë°ì´í„° ì €ì¥
    BA->>DL: ë°ì´í„° ì½ê¸°
    BA->>BA: ë³µì¡í•œ ë³€í™˜/ì§‘ê³„
    BA->>DW: ê²°ê³¼ ì €ì¥
    BI->>DW: ë¦¬í¬íŠ¸ ìƒì„±
```

**AWS ì„œë¹„ìŠ¤:**
- Amazon S3 (ë°ì´í„° ë ˆì´í¬)
- AWS Glue (ETL)
- Amazon EMR (ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬)
- Amazon Athena (SQL ì¿¼ë¦¬)

#### 2ï¸âƒ£ ì‹¤ì‹œê°„ ë¶„ì„ (Real-time Analysis)

**íŠ¹ì§•:**
- ë°ì´í„° ë°œìƒ ì¦‰ì‹œ ì²˜ë¦¬
- ë‚®ì€ ì§€ì—° ì‹œê°„ (ë°€ë¦¬ì´ˆ ~ ì´ˆ ë‹¨ìœ„)
- ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬

**í™œìš© ì‚¬ë¡€:**
- ë°°ì†¡ ê²½ë¡œ ìµœì í™”
- ë¶€ì • ê±°ë˜ íƒì§€
- ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ
- IoT ì„¼ì„œ ëª¨ë‹ˆí„°ë§

**í”„ë¡œì„¸ìŠ¤:**

```mermaid
sequenceDiagram
    participant IoT as IoT ê¸°ê¸°/ì• í”Œë¦¬ì¼€ì´ì…˜
    participant K as Kinesis Stream
    participant Lambda as Lambda í•¨ìˆ˜
    participant ES as ElastiCache
    participant App as ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

    IoT->>K: ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
    K->>Lambda: ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
    Lambda->>Lambda: ë°ì´í„° ì²˜ë¦¬/ë¶„ì„
    Lambda->>ES: ê²°ê³¼ ì €ì¥
    App->>ES: ì‹¤ì‹œê°„ ì¡°íšŒ
    App->>App: ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
```

**AWS ì„œë¹„ìŠ¤:**
- Amazon Kinesis (ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ìˆ˜ì§‘)
- AWS Lambda (ì‹¤ì‹œê°„ ì²˜ë¦¬)
- Amazon ElastiCache (ì €ì§€ì—° ì €ì¥ì†Œ)
- Amazon OpenSearch (ì‹¤ì‹œê°„ ê²€ìƒ‰/ë¶„ì„)

#### 3ï¸âƒ£ ì‹œê°í™” (Visualization)

**ì •ì˜:**
ë°ì´í„°ë¥¼ **ê·¸ë˜í”„, ì°¨íŠ¸, ëŒ€ì‹œë³´ë“œ** í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì´í•´í•˜ê¸° ì‰½ê²Œ í‘œí˜„í•˜ëŠ” ê³¼ì •

**ì‹œê°í™”ì˜ ì¤‘ìš”ì„±:**

| ì¥ì  | ì„¤ëª… |
|------|------|
| **ì§ê´€ì„±** | ë³µì¡í•œ ë°ì´í„°ë¥¼ í•œëˆˆì— íŒŒì•… |
| **íŒ¨í„´ ë°œê²¬** | ìˆ¨ê²¨ì§„ íŠ¸ë Œë“œì™€ ì´ìƒì¹˜ ì‹ë³„ |
| **ì»¤ë®¤ë‹ˆì¼€ì´ì…˜** | ë¹„ì „ë¬¸ê°€ë„ ì´í•´ ê°€ëŠ¥ |
| **ì˜ì‚¬ê²°ì • ì†ë„** | ì‹ ì†í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ |

**ì‹œê°í™” ìœ í˜•:**

```mermaid
graph TD
    A[ë°ì´í„° ì‹œê°í™”] --> B[ì°¨íŠ¸]
    A --> C[ê·¸ë˜í”„]
    A --> D[ëŒ€ì‹œë³´ë“œ]
    A --> E[ì§€ë„]

    B --> B1[ë§‰ëŒ€ ì°¨íŠ¸]
    B --> B2[ì› ì°¨íŠ¸]
    B --> B3[ì„  ì°¨íŠ¸]

    C --> C1[íˆìŠ¤í† ê·¸ë¨]
    C --> C2[ì‚°ì ë„]
    C --> C3[ë°•ìŠ¤ í”Œë¡¯]

    D --> D1[KPI ëŒ€ì‹œë³´ë“œ]
    D --> D2[ìš´ì˜ ëŒ€ì‹œë³´ë“œ]

    E --> E1[íˆíŠ¸ë§µ]
    E --> E2[ì§€ë¦¬ì  ë¶„í¬ë„]
```

**AWS ì‹œê°í™” ë„êµ¬:**
- Amazon QuickSight (BI ë„êµ¬)
- Amazon OpenSearch (Kibana ëŒ€ì‹œë³´ë“œ)
- AWS Grafana (ë©”íŠ¸ë¦­ ì‹œê°í™”)

---

### ğŸ’¾ ë°ì´í„° ë ˆì´í¬ (Data Lake)

**ì •ì˜:**
ë‹¤ì–‘í•œ í˜•ì‹ì˜ **ì›ì‹œ ë°ì´í„°(Raw Data)**ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ëŠ” ì¤‘ì•™ ì§‘ì¤‘ì‹ ì €ì¥ì†Œ

**íŠ¹ì§•:**

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ìŠ¤í‚¤ë§ˆ-ì˜¨-ë¦¬ë“œ** | ë°ì´í„° ì €ì¥ ì‹œ ìŠ¤í‚¤ë§ˆ ì •ì˜ ë¶ˆí•„ìš”<br>ì½ì„ ë•Œ ìŠ¤í‚¤ë§ˆ ì ìš© |
| **ëª¨ë“  í˜•ì‹** | ì •í˜•, ë°˜ì •í˜•, ë¹„ì •í˜• ë°ì´í„° ì €ì¥ |
| **ì €ë ´í•œ ë¹„ìš©** | S3 ê°™ì€ ê°ì²´ ìŠ¤í† ë¦¬ì§€ í™œìš© |
| **í™•ì¥ì„±** | í˜íƒ€ë°”ì´íŠ¸ ê·œëª¨ê¹Œì§€ í™•ì¥ ê°€ëŠ¥ |

**ë°ì´í„° ë ˆì´í¬ ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    subgraph Sources[ë°ì´í„° ì†ŒìŠ¤]
        A1[ì›¹ ë¡œê·¸]
        A2[IoT ì„¼ì„œ]
        A3[ëª¨ë°”ì¼ ì•±]
        A4[ë°ì´í„°ë² ì´ìŠ¤]
    end

    subgraph Lake[ë°ì´í„° ë ˆì´í¬ - S3]
        B1[Raw Zone<br>ì›ì‹œ ë°ì´í„°]
        B2[Processed Zone<br>ì •ì œ ë°ì´í„°]
        B3[Curated Zone<br>ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°]
    end

    subgraph Analytics[ë¶„ì„ ë„êµ¬]
        C1[Athena]
        C2[EMR]
        C3[Redshift Spectrum]
    end

    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1

    B1 --> B2
    B2 --> B3

    B1 --> C1
    B2 --> C2
    B3 --> C3

    style Lake fill:#e3f2fd
    style Sources fill:#fff3e0
    style Analytics fill:#e8f5e9
```

**S3ë¥¼ ë°ì´í„° ë ˆì´í¬ë¡œ ì‚¬ìš©í•˜ëŠ” ì´ìœ :**

1. **ì €ë ´í•œ ë¹„ìš©**
```
ë¹„ìš© ë¹„êµ:
- EBS gp3: $0.08/GB/ì›”
- S3 Standard: $0.023/GB/ì›”
- S3 IA: $0.0125/GB/ì›”
- S3 Glacier: $0.004/GB/ì›”
```

2. **ìë™ í™•ì¥**
- ìš©ëŸ‰ ì œí•œ ì—†ìŒ
- ìˆ˜ë™ ê´€ë¦¬ ë¶ˆí•„ìš”
- ìë™ ë‚´êµ¬ì„± ë³´ì¥ (99.999999999%)

3. **S3 ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ í™œìš©**

| í´ë˜ìŠ¤ | ì‚¬ìš© ë¹ˆë„ | ë¹„ìš© | í™œìš© ì‚¬ë¡€ |
|--------|-----------|------|-----------|
| **Standard** | ìì£¼ ì•¡ì„¸ìŠ¤ | $$$ | í™œì„± ë¶„ì„ ë°ì´í„° |
| **Intelligent-Tiering** | ìë™ ìµœì í™” | $$-$ | ì•¡ì„¸ìŠ¤ íŒ¨í„´ ë¶ˆëª…í™• |
| **Standard-IA** | ê°€ë” ì•¡ì„¸ìŠ¤ | $$ | ì›”ê°„ ë¶„ì„ ë°ì´í„° |
| **One Zone-IA** | ê°€ë” ì•¡ì„¸ìŠ¤ | $ | ì¬ìƒì„± ê°€ëŠ¥ ë°ì´í„° |
| **Glacier Instant Retrieval** | ë¶„ê¸°ë³„ ì•¡ì„¸ìŠ¤ | $ | ì¥ê¸° ë³´ê´€ |
| **Glacier Flexible Retrieval** | ì—° 1-2íšŒ | $ | ê·œì • ì¤€ìˆ˜ ë³´ê´€ |
| **Glacier Deep Archive** | ê±°ì˜ ì•¡ì„¸ìŠ¤ ì•ˆí•¨ | $ | ì¥ê¸° ì•„ì¹´ì´ë¸Œ |

**S3 Lifecycle ì •ì±… ì˜ˆì‹œ:**

```json
{
  "Rules": [
    {
      "Id": "MoveToIA",
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 90,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 365,
          "StorageClass": "GLACIER"
        }
      ]
    }
  ]
}
```

**ë°ì´í„° ë ˆì´í¬ êµ¬ì¶• ì˜ˆì‹œ:**

```bash
# 1. S3 ë²„í‚· ìƒì„± (ë°ì´í„° ë ˆì´í¬)
aws s3 mb s3://my-data-lake --region us-east-1

# 2. ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
aws s3api put-object --bucket my-data-lake --key raw/
aws s3api put-object --bucket my-data-lake --key processed/
aws s3api put-object --bucket my-data-lake --key curated/

# 3. ë°ì´í„° ì—…ë¡œë“œ
aws s3 cp local-data/ s3://my-data-lake/raw/ --recursive

# 4. Lifecycle ì •ì±… ì ìš©
aws s3api put-bucket-lifecycle-configuration \
    --bucket my-data-lake \
    --lifecycle-configuration file://lifecycle.json

# 5. ë²„ì „ ê´€ë¦¬ í™œì„±í™”
aws s3api put-bucket-versioning \
    --bucket my-data-lake \
    --versioning-configuration Status=Enabled
```

---

### ğŸ¢ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ (Data Warehouse)

**ì •ì˜:**
ë¶„ì„ì„ ìœ„í•´ **ì •ì œë˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°**ë¥¼ ì €ì¥í•˜ëŠ” ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤

**ë°ì´í„° ë ˆì´í¬ vs ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤:**

| êµ¬ë¶„ | ë°ì´í„° ë ˆì´í¬ | ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ |
|------|---------------|-------------------|
| **ë°ì´í„° í˜•ì‹** | ì›ì‹œ ë°ì´í„° (ëª¨ë“  í˜•ì‹) | ì •ì œëœ ë°ì´í„° (êµ¬ì¡°í™”) |
| **ìŠ¤í‚¤ë§ˆ** | Schema-on-read | Schema-on-write |
| **ì‚¬ìš©ì** | ë°ì´í„° ê³¼í•™ì, ì—”ì§€ë‹ˆì–´ | ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ |
| **ëª©ì ** | íƒìƒ‰ì  ë¶„ì„, ML | ë¦¬í¬íŒ…, BI |
| **ìŠ¤í† ë¦¬ì§€** | S3 (ê°ì²´ ìŠ¤í† ë¦¬ì§€) | Redshift (ì»¬ëŸ¼ ìŠ¤í† ë¦¬ì§€) |
| **ë¹„ìš©** | ì €ë ´ | ë¹„êµì  ë†’ìŒ |
| **ì¿¼ë¦¬ ì†ë„** | ëŠë¦¼ (ëŒ€ìš©ëŸ‰) | ë¹ ë¦„ (ìµœì í™”) |

**ë°ì´í„° íë¦„:**

```mermaid
flowchart LR
    A[ë°ì´í„° ì†ŒìŠ¤] -->|ì›ì‹œ ë°ì´í„°| B[ë°ì´í„° ë ˆì´í¬<br>S3]
    B -->|ETL| C{ë°ì´í„° ë³€í™˜}
    C -->|ì •ì œ/ì§‘ê³„| D[ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤<br>Redshift]
    C -->|í•„ìš”ì‹œ| B
    D --> E[BI ë„êµ¬<br>QuickSight]
    D --> F[SQL ì¿¼ë¦¬]

    style A fill:#fff9c4
    style B fill:#e1f5ff
    style D fill:#e8f5e9
    style E fill:#fce4ec
```

**ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ íŠ¹ì§•:**

1. **Star Schema ì„¤ê³„**
```
ì‚¬ì‹¤ í…Œì´ë¸” (Fact Table)
â””â”€â”€ ì°¨ì› í…Œì´ë¸” (Dimension Tables)
    â”œâ”€â”€ ì‹œê°„ ì°¨ì›
    â”œâ”€â”€ ì œí’ˆ ì°¨ì›
    â”œâ”€â”€ ê³ ê° ì°¨ì›
    â””â”€â”€ ì§€ì—­ ì°¨ì›
```

2. **ì»¬ëŸ¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€**
```
í–‰ ê¸°ë°˜ (OLTP):
[ID: 1, Name: "John", Age: 30, City: "Seoul"]
[ID: 2, Name: "Jane", Age: 25, City: "Busan"]

ì»¬ëŸ¼ ê¸°ë°˜ (OLAP):
ID:   [1, 2]
Name: ["John", "Jane"]
Age:  [30, 25]
City: ["Seoul", "Busan"]
```
â†’ íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ì–´ ë¶„ì„ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ

3. **ì••ì¶• ë° ì¸ì½”ë”©**
- ì»¬ëŸ¼ë³„ ìµœì  ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì ìš©
- ì €ì¥ ê³µê°„ ì ˆê° (ìµœëŒ€ 90%)
- ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ

**Redshift ì•„í‚¤í…ì²˜ ë³µìŠµ:**

```mermaid
graph TD
    A[í´ë¼ì´ì–¸íŠ¸] -->|SQL ì¿¼ë¦¬| B[ë¦¬ë” ë…¸ë“œ]
    B -->|ì¿¼ë¦¬ ë¶„ì‚°| C[ì»´í“¨íŒ… ë…¸ë“œ 1]
    B -->|ì¿¼ë¦¬ ë¶„ì‚°| D[ì»´í“¨íŒ… ë…¸ë“œ 2]
    B -->|ì¿¼ë¦¬ ë¶„ì‚°| E[ì»´í“¨íŒ… ë…¸ë“œ N]

    C -->|ë¡œì»¬ ìŠ¤í† ë¦¬ì§€| C1[ë°ì´í„° ìŠ¬ë¼ì´ìŠ¤]
    D -->|ë¡œì»¬ ìŠ¤í† ë¦¬ì§€| D1[ë°ì´í„° ìŠ¬ë¼ì´ìŠ¤]
    E -->|ë¡œì»¬ ìŠ¤í† ë¦¬ì§€| E1[ë°ì´í„° ìŠ¬ë¼ì´ìŠ¤]

    C -.->|Cold ë°ì´í„°| F[S3]
    D -.->|Cold ë°ì´í„°| F
    E -.->|Cold ë°ì´í„°| F

    C1 --> B
    D1 --> B
    E1 --> B
    B -->|ê²°ê³¼ ë°˜í™˜| A

    style B fill:#ff9999
    style C fill:#99ccff
    style D fill:#99ccff
    style E fill:#99ccff
    style F fill:#99ff99
```

---

### ğŸ”„ ETL vs ELT

**ETL (Extract, Transform, Load):**

```mermaid
sequenceDiagram
    participant S as ë°ì´í„° ì†ŒìŠ¤
    participant E as ETL ì—”ì§„
    participant T as ë³€í™˜ ì„œë²„
    participant W as ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤

    S->>E: 1. Extract (ì¶”ì¶œ)
    E->>T: 2. Transform (ë³€í™˜)
    Note over T: - ì •ì œ<br>- ì§‘ê³„<br>- ì¡°ì¸
    T->>W: 3. Load (ì ì¬)
```

**ELT (Extract, Load, Transform):**

```mermaid
sequenceDiagram
    participant S as ë°ì´í„° ì†ŒìŠ¤
    participant E as ELT ì—”ì§„
    participant W as ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤
    participant T as ì›¨ì–´í•˜ìš°ìŠ¤ ë‚´ë¶€

    S->>E: 1. Extract (ì¶”ì¶œ)
    E->>W: 2. Load (ì ì¬)
    W->>T: 3. Transform (ë³€í™˜)
    Note over T: ì›¨ì–´í•˜ìš°ìŠ¤ì˜<br>ì»´í“¨íŒ… íŒŒì›Œ í™œìš©
```

**ë¹„êµ:**

| êµ¬ë¶„ | ETL | ELT |
|------|-----|-----|
| **ë³€í™˜ ìœ„ì¹˜** | ì™¸ë¶€ ETL ì„œë²„ | ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë‚´ë¶€ |
| **ì í•© í™˜ê²½** | ì „í†µì  DW | í´ë¼ìš°ë“œ DW (Redshift, Snowflake) |
| **ë¹„ìš©** | ETL ì„œë²„ ë¹„ìš© | DW ì»´í“¨íŒ… ë¹„ìš© |
| **í™•ì¥ì„±** | ETL ì„œë²„ í™•ì¥ í•„ìš” | DW ìë™ í™•ì¥ |
| **ì†ë„** | ëŠë¦¼ (ë³„ë„ ë³€í™˜) | ë¹ ë¦„ (ë³‘ë ¬ ì²˜ë¦¬) |

ğŸ’¡ **ì¤‘ìš”!**: ìµœê·¼ í´ë¼ìš°ë“œ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ê°•ë ¥í•œ ì»´í“¨íŒ… íŒŒì›Œë¡œ ì¸í•´ ELT íŒ¨í„´ì´ ì„ í˜¸ë˜ê³  ìˆìŠµë‹ˆë‹¤.

---

### âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ë°ì´í„° ë¶„ì„ ê°œë…:**
- [ ] ë°ì´í„° ë¶„ì„ê³¼ ê¸°ê³„í•™ìŠµì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ë°°ì¹˜ ë¶„ì„ê³¼ ì‹¤ì‹œê°„ ë¶„ì„ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤
- [ ] ë°ì´í„° ì‹œê°í™”ì˜ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤

**ë°ì´í„° ë ˆì´í¬:**
- [ ] ë°ì´í„° ë ˆì´í¬ì˜ ê°œë…ê³¼ ëª©ì ì„ ì´í•´í•œë‹¤
- [ ] S3ë¥¼ ë°ì´í„° ë ˆì´í¬ë¡œ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] S3 ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ì˜ íŠ¹ì§•ê³¼ ë¹„ìš©ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤
- [ ] Lifecycle ì •ì±…ì„ ì‘ì„±í•  ìˆ˜ ìˆë‹¤

**ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤:**
- [ ] ë°ì´í„° ë ˆì´í¬ì™€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ì»¬ëŸ¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ì˜ ì¥ì ì„ ì´í•´í•œë‹¤
- [ ] Star Schemaì˜ ê°œë…ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ETLê³¼ ELTì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤

### ğŸ“‹ í•µì‹¬ ìš”ì•½

1. **ë°ì´í„° ë¶„ì„**: ê³¼ê±° ë°ì´í„°ì—ì„œ íŒ¨í„´ ë°œê²¬ â†’ ì˜ì‚¬ê²°ì • ì§€ì›
2. **ê¸°ê³„í•™ìŠµ**: ê³¼ê±° ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ â†’ ë¯¸ë˜ ì˜ˆì¸¡
3. **ë°°ì¹˜ ë¶„ì„**: ëŒ€ëŸ‰ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ (ì‹œê°„ ~ ì¼ ë‹¨ìœ„)
4. **ì‹¤ì‹œê°„ ë¶„ì„**: ë°ì´í„° ë°œìƒ ì¦‰ì‹œ ì²˜ë¦¬ (ë°€ë¦¬ì´ˆ ~ ì´ˆ ë‹¨ìœ„)
5. **ë°ì´í„° ë ˆì´í¬**: ì›ì‹œ ë°ì´í„°ë¥¼ ì €ë ´í•˜ê²Œ ëŒ€ëŸ‰ ì €ì¥ (S3)
6. **ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤**: ì •ì œëœ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì¿¼ë¦¬ (Redshift)
7. **ETL**: ë³€í™˜ í›„ ì ì¬ (ì „í†µì  ë°©ì‹)
8. **ELT**: ì ì¬ í›„ ë³€í™˜ (í´ë¼ìš°ë“œ í™˜ê²½)

---

## ğŸ”¬ AWS ë¶„ì„ ì„œë¹„ìŠ¤ ìƒì„¸

### ğŸ“Š Amazon Athena - ì„œë²„ë¦¬ìŠ¤ SQL ì¿¼ë¦¬ ì„œë¹„ìŠ¤

**ê°œë…:**
Amazon AthenaëŠ” **S3ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ SQLë¡œ ì§ì ‘ ì¿¼ë¦¬**í•  ìˆ˜ ìˆëŠ” ì„œë²„ë¦¬ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ì„œë²„ë¦¬ìŠ¤** | ì¸í”„ë¼ ê´€ë¦¬ ë¶ˆí•„ìš” |
| **í‘œì¤€ SQL** | ANSI SQL ì§€ì› (Presto ê¸°ë°˜) |
| **í˜ì´ í¼ ìŠ¤ìº”** | ìŠ¤ìº”í•œ ë°ì´í„°ì–‘ë§Œí¼ë§Œ ë¹„ìš© ì§€ë¶ˆ |
| **ë‹¤ì–‘í•œ í˜•ì‹** | CSV, JSON, ORC, Parquet, Avro ì§€ì› |
| **í†µí•©** | Glue Data Catalogì™€ ì™„ë²½ í†µí•© |

**Athena ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    A[ì‚¬ìš©ì] -->|SQL ì¿¼ë¦¬| B[Athena]
    B <-->|ë©”íƒ€ë°ì´í„°| C[Glue Data Catalog]
    B -->|ë°ì´í„° ì½ê¸°| D[S3 ë²„í‚·]

    C --> C1[í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ]
    C --> C2[íŒŒí‹°ì…˜ ì •ë³´]

    D --> D1[CSV íŒŒì¼]
    D --> D2[JSON íŒŒì¼]
    D --> D3[Parquet íŒŒì¼]

    B -->|ê²°ê³¼ ì €ì¥| E[S3 ê²°ê³¼ ë²„í‚·]

    style B fill:#ff9999
    style C fill:#99ccff
    style D fill:#99ff99
```

**ì‹¤ìŠµ: Athenaë¡œ S3 ë°ì´í„° ì¿¼ë¦¬í•˜ê¸°**

**Step 1: S3 ë²„í‚· ì¤€ë¹„**
```bash
# 1. S3 ë²„í‚· ìƒì„±
aws s3 mb s3://my-athena-data-bucket

# 2. ìƒ˜í”Œ ë°ì´í„° ì—…ë¡œë“œ (CSV)
cat > sample_logs.csv << EOF
timestamp,user_id,action,status_code
2025-12-11T10:00:00,user1,login,200
2025-12-11T10:05:00,user2,purchase,200
2025-12-11T10:10:00,user1,logout,200
2025-12-11T10:15:00,user3,login,401
EOF

aws s3 cp sample_logs.csv s3://my-athena-data-bucket/logs/

# 3. Athena ê²°ê³¼ ë²„í‚· ìƒì„±
aws s3 mb s3://my-athena-results
```

**Step 2: Athenaì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„±**
```sql
-- ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
CREATE DATABASE my_analytics_db;

-- ì™¸ë¶€ í…Œì´ë¸” ìƒì„±
CREATE EXTERNAL TABLE IF NOT EXISTS my_analytics_db.web_logs (
    timestamp STRING,
    user_id STRING,
    action STRING,
    status_code INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 's3://my-athena-data-bucket/logs/'
TBLPROPERTIES ('skip.header.line.count'='1');
```

**Step 3: ì¿¼ë¦¬ ì‹¤í–‰**
```sql
-- 1. ì „ì²´ ë°ì´í„° ì¡°íšŒ
SELECT * FROM my_analytics_db.web_logs
LIMIT 10;

-- 2. ì•¡ì…˜ë³„ ì§‘ê³„
SELECT 
    action,
    COUNT(*) as count
FROM my_analytics_db.web_logs
GROUP BY action
ORDER BY count DESC;

-- 3. ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„ì„
SELECT 
    SUBSTR(timestamp, 1, 13) as hour,
    COUNT(*) as requests
FROM my_analytics_db.web_logs
GROUP BY SUBSTR(timestamp, 1, 13)
ORDER BY hour;

-- 4. ì—ëŸ¬ ë¡œê·¸ í•„í„°ë§
SELECT 
    timestamp,
    user_id,
    action,
    status_code
FROM my_analytics_db.web_logs
WHERE status_code >= 400;
```

**íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”:**

```sql
-- íŒŒí‹°ì…˜ëœ í…Œì´ë¸” ìƒì„±
CREATE EXTERNAL TABLE my_analytics_db.web_logs_partitioned (
    timestamp STRING,
    user_id STRING,
    action STRING,
    status_code INT
)
PARTITIONED BY (
    year STRING,
    month STRING,
    day STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 's3://my-athena-data-bucket/logs_partitioned/';

-- íŒŒí‹°ì…˜ ì¶”ê°€
ALTER TABLE my_analytics_db.web_logs_partitioned 
ADD PARTITION (year='2025', month='12', day='11')
LOCATION 's3://my-athena-data-bucket/logs_partitioned/year=2025/month=12/day=11/';

-- íŒŒí‹°ì…˜ ì¿¼ë¦¬ (ìŠ¤ìº” ë²”ìœ„ ì¶•ì†Œ)
SELECT * 
FROM my_analytics_db.web_logs_partitioned
WHERE year='2025' AND month='12' AND day='11';
```

**ë¹„ìš© ìµœì í™” íŒ:**

1. **Parquet í˜•ì‹ ì‚¬ìš©**
```sql
-- CSVë¥¼ Parquetë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
CREATE TABLE my_analytics_db.web_logs_parquet
WITH (
    format='PARQUET',
    parquet_compression='SNAPPY',
    external_location='s3://my-athena-data-bucket/logs_parquet/'
) AS
SELECT * FROM my_analytics_db.web_logs;
```
â†’ ParquetëŠ” ì»¬ëŸ¼ ê¸°ë°˜ í˜•ì‹ìœ¼ë¡œ ì••ì¶•ë¥ ì´ ë†’ê³  ì¿¼ë¦¬ ì†ë„ê°€ ë¹ ë¦„

2. **ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œì™¸**
```sql
-- âŒ ë¹„íš¨ìœ¨ì 
SELECT * FROM large_table;

-- âœ… íš¨ìœ¨ì 
SELECT user_id, action FROM large_table;
```

3. **LIMIT ì‚¬ìš©**
```sql
-- íƒìƒ‰ì  ë¶„ì„ ì‹œ LIMITë¡œ ìŠ¤ìº” ë²”ìœ„ ì œí•œ
SELECT * FROM large_table LIMIT 1000;
```

---

### ğŸ”„ AWS Glue - ì„œë²„ë¦¬ìŠ¤ ETL ì„œë¹„ìŠ¤

**ê°œë…:**
AWS GlueëŠ” **ë°ì´í„°ë¥¼ ì¶”ì¶œ(Extract), ë³€í™˜(Transform), ì ì¬(Load)**í•˜ëŠ” ì™„ì „ ê´€ë¦¬í˜• ETL ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ì£¼ìš” êµ¬ì„± ìš”ì†Œ:**

| êµ¬ì„± ìš”ì†Œ | ì—­í•  |
|-----------|------|
| **Glue Data Catalog** | ì¤‘ì•™ ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ |
| **Glue Crawler** | ìë™ ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰ ë° ë“±ë¡ |
| **Glue ETL Jobs** | ë°ì´í„° ë³€í™˜ ì‘ì—… |
| **Glue Triggers** | ì‘ì—… ìŠ¤ì¼€ì¤„ë§ |
| **Glue Workflows** | ì—¬ëŸ¬ ì‘ì—… ì¡°ìœ¨ |

**Glue ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    A[ë°ì´í„° ì†ŒìŠ¤] --> B[Glue Crawler]
    B -->|ìŠ¤í‚¤ë§ˆ ê²€ìƒ‰| C[Glue Data Catalog]
    C -->|ë©”íƒ€ë°ì´í„°| D[Glue ETL Job]
    A -->|ì›ì‹œ ë°ì´í„°| D
    D -->|ë³€í™˜ëœ ë°ì´í„°| E[ë°ì´í„° ëŒ€ìƒ]

    subgraph Sources[ë°ì´í„° ì†ŒìŠ¤]
        A1[S3]
        A2[RDS]
        A3[DynamoDB]
    end

    subgraph Targets[ë°ì´í„° ëŒ€ìƒ]
        E1[S3]
        E2[Redshift]
        E3[RDS]
    end

    A1 --> A
    A2 --> A
    A3 --> A

    E --> E1
    E --> E2
    E --> E3

    style C fill:#ff9999
    style D fill:#99ccff
```

**ì‹¤ìŠµ: Glueë¡œ ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**

**Step 1: Glue Crawler ìƒì„±**

```bash
# AWS CLIë¥¼ ì‚¬ìš©í•œ Crawler ìƒì„±
aws glue create-crawler \
    --name my-s3-crawler \
    --role AWSGlueServiceRole \
    --database-name my_glue_db \
    --targets '{
        "S3Targets": [
            {
                "Path": "s3://my-data-bucket/input/"
            }
        ]
    }' \
    --schema-change-policy '{
        "UpdateBehavior": "UPDATE_IN_DATABASE",
        "DeleteBehavior": "LOG"
    }'

# Crawler ì‹¤í–‰
aws glue start-crawler --name my-s3-crawler

# Crawler ìƒíƒœ í™•ì¸
aws glue get-crawler --name my-s3-crawler
```

**Step 2: Glue ETL Job ìƒì„± (Python Shell)**

```python
# glue_etl_job.py
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

## íŒŒë¼ë¯¸í„° ì½ê¸°
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path', 'output_path'])

## Glue Context ì´ˆê¸°í™”
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

## ë°ì´í„° ì½ê¸°
datasource0 = glueContext.create_dynamic_frame.from_catalog(
    database="my_glue_db",
    table_name="input_table"
)

## ë°ì´í„° ë³€í™˜
# 1. ì»¬ëŸ¼ ì„ íƒ
transformed = SelectFields.apply(
    frame=datasource0,
    paths=["user_id", "timestamp", "action", "amount"]
)

# 2. ë°ì´í„° í•„í„°ë§
filtered = Filter.apply(
    frame=transformed,
    f=lambda x: x["amount"] > 100
)

# 3. ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
renamed = RenameField.apply(
    frame=filtered,
    old_name="amount",
    new_name="transaction_amount"
)

# 4. ì§‘ê³„
aggregated = renamed.toDF().groupBy("user_id").agg({
    "transaction_amount": "sum"
})

## ë°ì´í„° ì €ì¥
glueContext.write_dynamic_frame.from_options(
    frame=DynamicFrame.fromDF(aggregated, glueContext, "aggregated"),
    connection_type="s3",
    connection_options={
        "path": args['output_path']
    },
    format="parquet"
)

job.commit()
```

**Step 3: Glue Job ì‹¤í–‰**

```bash
# Glue Job ìƒì„±
aws glue create-job \
    --name my-etl-job \
    --role AWSGlueServiceRole \
    --command '{
        "Name": "glueetl",
        "ScriptLocation": "s3://my-glue-scripts/glue_etl_job.py",
        "PythonVersion": "3"
    }' \
    --default-arguments '{
        "--input_path": "s3://my-data-bucket/input/",
        "--output_path": "s3://my-data-bucket/output/"
    }' \
    --glue-version "4.0"

# Job ì‹¤í–‰
aws glue start-job-run \
    --job-name my-etl-job

# Job ì‹¤í–‰ ìƒíƒœ í™•ì¸
aws glue get-job-run \
    --job-name my-etl-job \
    --run-id <run-id>
```

**Glue ETL ë³€í™˜ ì˜ˆì œ:**

```python
# 1. ì¡°ì¸ (Join)
joined = Join.apply(
    frame1=customers,
    frame2=orders,
    keys1=["customer_id"],
    keys2=["customer_id"]
)

# 2. ì¤‘ë³µ ì œê±° (Deduplicate)
deduped = DynamicFrame.fromDF(
    customers.toDF().dropDuplicates(["email"]),
    glueContext,
    "deduped"
)

# 3. ë°ì´í„° íƒ€ì… ë³€í™˜
from pyspark.sql.functions import col
df = data.toDF()
df = df.withColumn("amount", col("amount").cast("double"))
df = df.withColumn("date", col("date").cast("date"))

# 4. íŒŒí‹°ì…˜ ì‘ì„±
glueContext.write_dynamic_frame.from_options(
    frame=data,
    connection_type="s3",
    connection_options={
        "path": "s3://output-bucket/data/",
        "partitionKeys": ["year", "month", "day"]
    },
    format="parquet"
)
```

---

### ğŸ” Amazon OpenSearch (êµ¬ ElasticSearch) - ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„

**ê°œë…:**
Amazon OpenSearchëŠ” **ë¡œê·¸ ë¶„ì„, ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§, í´ë¦­ìŠ¤íŠ¸ë¦¼ ë¶„ì„**ì„ ìœ„í•œ ì™„ì „ ê´€ë¦¬í˜• ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**

- ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ (Elasticsearch, Kibana í˜¸í™˜)
- ì‹¤ì‹œê°„ ê²€ìƒ‰ ë° ë¶„ì„
- ë‚´ì¥ ì‹œê°í™” ë„êµ¬ (OpenSearch Dashboards)
- ê³ ê°€ìš©ì„± (Multi-AZ ë°°í¬)

**ELK ìŠ¤íƒ:**

```mermaid
graph TD
    A[ë°ì´í„° ì†ŒìŠ¤] --> B[Logstash<br>ë°ì´í„° ìˆ˜ì§‘/ë³€í™˜]
    B --> C[Elasticsearch<br>ê²€ìƒ‰/ë¶„ì„ ì—”ì§„]
    C --> D[Kibana<br>ì‹œê°í™” ëŒ€ì‹œë³´ë“œ]

    subgraph Sources[ë°ì´í„° ì†ŒìŠ¤]
        A1[ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸]
        A2[ì‹œìŠ¤í…œ ë¡œê·¸]
        A3[ì›¹ ì„œë²„ ë¡œê·¸]
    end

    A1 --> A
    A2 --> A
    A3 --> A

    style B fill:#99ccff
    style C fill:#99ff99
    style D fill:#ffcccc
```

**ì‹¤ìŠµ: OpenSearch ë„ë©”ì¸ ìƒì„± ë° ë°ì´í„° ì¸ë±ì‹±**

**Step 1: OpenSearch ë„ë©”ì¸ ìƒì„±**

```bash
# AWS CLIë¡œ OpenSearch ë„ë©”ì¸ ìƒì„±
aws opensearch create-domain \
    --domain-name my-logs-domain \
    --engine-version "OpenSearch_2.9" \
    --cluster-config '{
        "InstanceType": "t3.small.search",
        "InstanceCount": 2,
        "DedicatedMasterEnabled": false,
        "ZoneAwarenessEnabled": true
    }' \
    --ebs-options '{
        "EBSEnabled": true,
        "VolumeType": "gp3",
        "VolumeSize": 20
    }' \
    --access-policies '{
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {"AWS": "*"},
                "Action": "es:*",
                "Resource": "arn:aws:es:region:account-id:domain/my-logs-domain/*",
                "Condition": {
                    "IpAddress": {
                        "aws:SourceIp": "YOUR_IP_ADDRESS"
                    }
                }
            }
        ]
    }'

# ë„ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
aws opensearch describe-domain \
    --domain-name my-logs-domain \
    --query 'DomainStatus.Endpoint' \
    --output text
```

**Step 2: ë°ì´í„° ì¸ë±ì‹±**

```bash
# OpenSearch ì—”ë“œí¬ì¸íŠ¸
ENDPOINT="https://my-logs-domain-xxx.us-east-1.es.amazonaws.com"

# ì¸ë±ìŠ¤ ìƒì„±
curl -X PUT "${ENDPOINT}/weblogs" \
    -H 'Content-Type: application/json' \
    -d '{
        "settings": {
            "number_of_shards": 2,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                "timestamp": {"type": "date"},
                "user_id": {"type": "keyword"},
                "action": {"type": "keyword"},
                "ip_address": {"type": "ip"},
                "user_agent": {"type": "text"},
                "status_code": {"type": "integer"},
                "response_time": {"type": "float"}
            }
        }
    }'

# ë¬¸ì„œ ìƒ‰ì¸ (ë‹¨ì¼ ë¬¸ì„œ)
curl -X POST "${ENDPOINT}/weblogs/_doc" \
    -H 'Content-Type: application/json' \
    -d '{
        "timestamp": "2025-12-11T10:00:00",
        "user_id": "user123",
        "action": "login",
        "ip_address": "203.0.113.1",
        "user_agent": "Mozilla/5.0",
        "status_code": 200,
        "response_time": 0.15
    }'

# ë²Œí¬ ìƒ‰ì¸ (ì—¬ëŸ¬ ë¬¸ì„œ)
curl -X POST "${ENDPOINT}/_bulk" \
    -H 'Content-Type: application/x-ndjson' \
    --data-binary @bulk_data.ndjson
```

**Step 3: ë°ì´í„° ê²€ìƒ‰**

```bash
# 1. ì „ì²´ ê²€ìƒ‰
curl -X GET "${ENDPOINT}/weblogs/_search?pretty"

# 2. íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰
curl -X GET "${ENDPOINT}/weblogs/_search?pretty" \
    -H 'Content-Type: application/json' \
    -d '{
        "query": {
            "term": {
                "user_id": "user123"
            }
        }
    }'

# 3. ì‹œê°„ ë²”ìœ„ ê²€ìƒ‰
curl -X GET "${ENDPOINT}/weblogs/_search?pretty" \
    -H 'Content-Type: application/json' \
    -d '{
        "query": {
            "range": {
                "timestamp": {
                    "gte": "2025-12-11T00:00:00",
                    "lte": "2025-12-11T23:59:59"
                }
            }
        }
    }'

# 4. ì§‘ê³„ ì¿¼ë¦¬ (ì•¡ì…˜ë³„ ì¹´ìš´íŠ¸)
curl -X GET "${ENDPOINT}/weblogs/_search?pretty" \
    -H 'Content-Type: application/json' \
    -d '{
        "size": 0,
        "aggs": {
            "actions": {
                "terms": {
                    "field": "action",
                    "size": 10
                }
            }
        }
    }'

# 5. í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
curl -X GET "${ENDPOINT}/weblogs/_search?pretty" \
    -H 'Content-Type: application/json' \
    -d '{
        "size": 0,
        "aggs": {
            "avg_response_time": {
                "avg": {
                    "field": "response_time"
                }
            }
        }
    }'
```

---

### âš¡ Amazon EMR - ë¹…ë°ì´í„° ì²˜ë¦¬ í”Œë«í¼

**ê°œë…:**
Amazon EMR(Elastic MapReduce)ì€ **Hadoop, Spark ë“±ì˜ ë¹…ë°ì´í„° í”„ë ˆì„ì›Œí¬**ë¥¼ ì‹¤í–‰í•˜ëŠ” ê´€ë¦¬í˜• í´ëŸ¬ìŠ¤í„° í”Œë«í¼ì…ë‹ˆë‹¤.

**ì§€ì› í”„ë ˆì„ì›Œí¬:**

| í”„ë ˆì„ì›Œí¬ | ìš©ë„ |
|-----------|------|
| **Hadoop** | ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ ë° ì²˜ë¦¬ (MapReduce) |
| **Spark** | ê³ ì† ì¸ë©”ëª¨ë¦¬ ì²˜ë¦¬ |
| **Hive** | SQL ê¸°ë°˜ ë°ì´í„° ì¿¼ë¦¬ |
| **Presto** | ëŒ€í™”í˜• SQL ì¿¼ë¦¬ |
| **HBase** | NoSQL ë°ì´í„°ë² ì´ìŠ¤ |
| **Flink** | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ |

**EMR ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    A[Master Node<br>í´ëŸ¬ìŠ¤í„° ì¡°ìœ¨] --> B[Core Node 1<br>ë°ì´í„° ì €ì¥ + ì²˜ë¦¬]
    A --> C[Core Node 2<br>ë°ì´í„° ì €ì¥ + ì²˜ë¦¬]
    A --> D[Task Node 1<br>ì²˜ë¦¬ë§Œ]
    A --> E[Task Node 2<br>ì²˜ë¦¬ë§Œ]

    B <--> F[HDFS]
    C <--> F

    D -.->|ì‘ì—…ë§Œ ìˆ˜í–‰| G[ì²˜ë¦¬ ì‘ì—…]
    E -.->|ì‘ì—…ë§Œ ìˆ˜í–‰| G

    style A fill:#ff9999
    style B fill:#99ccff
    style C fill:#99ccff
    style D fill:#99ff99
    style E fill:#99ff99
```

**ì‹¤ìŠµ: EMR í´ëŸ¬ìŠ¤í„°ë¡œ Spark ì‘ì—… ì‹¤í–‰**

**Step 1: EMR í´ëŸ¬ìŠ¤í„° ìƒì„±**

```bash
# AWS CLIë¡œ EMR í´ëŸ¬ìŠ¤í„° ìƒì„±
aws emr create-cluster \
    --name "My Spark Cluster" \
    --release-label emr-6.14.0 \
    --applications Name=Spark Name=Hadoop \
    --ec2-attributes KeyName=my-key,SubnetId=subnet-xxx \
    --instance-type m5.xlarge \
    --instance-count 3 \
    --use-default-roles \
    --log-uri s3://my-emr-logs/ \
    --bootstrap-actions Path=s3://my-emr-bootstrap/install-packages.sh

# í´ëŸ¬ìŠ¤í„° ID í™•ì¸
aws emr list-clusters --active

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
aws emr describe-cluster --cluster-id j-XXXXXXXXXXXXX
```

**Step 2: Spark ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**

```python
# spark_job.py - S3ì— ì—…ë¡œë“œí•  Spark ì‘ì—…
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("LogAnalysis") \
    .getOrCreate()

# S3ì—ì„œ ë°ì´í„° ì½ê¸°
df = spark.read \
    .option("header", "true") \
    .csv("s3://my-data-bucket/logs/")

# ë°ì´í„° ë³€í™˜
df_transformed = df \
    .withColumn("hour", hour(col("timestamp"))) \
    .withColumn("date", to_date(col("timestamp")))

# ì‹œê°„ëŒ€ë³„ ìš”ì²­ ìˆ˜ ì§‘ê³„
hourly_stats = df_transformed \
    .groupBy("date", "hour") \
    .agg(
        count("*").alias("request_count"),
        avg("response_time").alias("avg_response_time")
    ) \
    .orderBy("date", "hour")

# ê²°ê³¼ ì €ì¥
hourly_stats.write \
    .mode("overwrite") \
    .partitionBy("date") \
    .parquet("s3://my-data-bucket/analytics/hourly_stats/")

# í†µê³„ ì¶œë ¥
print(f"Total records: {df.count()}")
print(f"Date range: {df.agg(min('timestamp'), max('timestamp')).collect()}")

spark.stop()
```

**Step 3: EMR Step ì¶”ê°€ (ì‘ì—… ì‹¤í–‰)**

```bash
# Spark ì‘ì—… ì œì¶œ
aws emr add-steps \
    --cluster-id j-XXXXXXXXXXXXX \
    --steps Type=Spark,Name="LogAnalysisJob",ActionOnFailure=CONTINUE,Args=[
        --deploy-mode,cluster,
        --master,yarn,
        --conf,spark.executor.memory=4g,
        --conf,spark.executor.cores=2,
        s3://my-emr-scripts/spark_job.py
    ]

# Step ìƒíƒœ í™•ì¸
aws emr list-steps --cluster-id j-XXXXXXXXXXXXX

# Step ë¡œê·¸ í™•ì¸ (S3)
aws s3 ls s3://my-emr-logs/j-XXXXXXXXXXXXX/steps/
```

---

### ğŸŒŠ Amazon Kinesis - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬

**ê°œë…:**
Amazon KinesisëŠ” **ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ë¶„ì„**í•˜ëŠ” ì„œë¹„ìŠ¤êµ°ì…ë‹ˆë‹¤.

**Kinesis ì„œë¹„ìŠ¤ êµ¬ì„±:**

| ì„œë¹„ìŠ¤ | ìš©ë„ |
|--------|------|
| **Kinesis Data Streams** | ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ |
| **Kinesis Data Firehose** | ë°ì´í„° ì „ì†¡ ë° ë³€í™˜ (ì¤€ì‹¤ì‹œê°„) |
| **Kinesis Data Analytics** | ì‹¤ì‹œê°„ SQL ë¶„ì„ |
| **Kinesis Video Streams** | ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° |

**Kinesis Data Streams ì•„í‚¤í…ì²˜:**

```mermaid
sequenceDiagram
    participant P1 as Producer 1<br>(ì›¹ì•±)
    participant P2 as Producer 2<br>(ëª¨ë°”ì¼)
    participant K as Kinesis Stream
    participant C1 as Consumer 1<br>(Lambda)
    participant C2 as Consumer 2<br>(EC2 ì•±)
    participant S as S3 / DynamoDB

    P1->>K: PutRecord (ì‹¤ì‹œê°„)
    P2->>K: PutRecord (ì‹¤ì‹œê°„)

    Note over K: ë°ì´í„° ì €ì¥<br>(ê¸°ë³¸ 24ì‹œê°„)

    K->>C1: GetRecords
    K->>C2: GetRecords

    C1->>S: ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
    C2->>S: ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
```

**Kinesis Data Firehose vs Data Streams:**

| êµ¬ë¶„ | Data Streams | Data Firehose |
|------|--------------|---------------|
| **ì§€ì—° ì‹œê°„** | ì‹¤ì‹œê°„ (< 1ì´ˆ) | ì¤€ì‹¤ì‹œê°„ (60ì´ˆ ë˜ëŠ” ë²„í¼) |
| **ë°ì´í„° ì €ì¥** | 24ì‹œê°„ ~ 365ì¼ | ë²„í¼ì—ë§Œ ì„ì‹œ ì €ì¥ |
| **ì†Œë¹„ì** | ì§ì ‘ êµ¬í˜„ í•„ìš” | ìë™ìœ¼ë¡œ ëŒ€ìƒì— ì „ì†¡ |
| **ë³€í™˜** | Lambda í†µí•© | ë‚´ì¥ ë³€í™˜ ê¸°ëŠ¥ |
| **ê´€ë¦¬** | ìƒ¤ë“œ ê´€ë¦¬ í•„ìš” | ì™„ì „ ê´€ë¦¬í˜• |
| **ë¹„ìš©** | ìƒ¤ë“œ ì‹œê°„ + PUT | ë°ì´í„° ì–‘ |

**ì‹¤ìŠµ: Kinesis Data Streamsë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬**

**Step 1: Kinesis Stream ìƒì„±**

```bash
# Stream ìƒì„± (ìƒ¤ë“œ 2ê°œ)
aws kinesis create-stream \
    --stream-name my-data-stream \
    --shard-count 2

# Stream ìƒíƒœ í™•ì¸
aws kinesis describe-stream \
    --stream-name my-data-stream
```

**Step 2: Producer - ë°ì´í„° ì „ì†¡**

```python
# kinesis_producer.py
import boto3
import json
import time
from datetime import datetime

kinesis = boto3.client('kinesis', region_name='us-east-1')
stream_name = 'my-data-stream'

def send_data(user_id, action, value):
    data = {
        'timestamp': datetime.utcnow().isoformat(),
        'user_id': user_id,
        'action': action,
        'value': value
    }

    response = kinesis.put_record(
        StreamName=stream_name,
        Data=json.dumps(data),
        PartitionKey=user_id  # ê°™ì€ user_idëŠ” ê°™ì€ ìƒ¤ë“œë¡œ
    )

    print(f"Sent: {data}, ShardId: {response['ShardId']}")

# ì§€ì†ì ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
while True:
    send_data('user_123', 'click', 1)
    send_data('user_456', 'purchase', 99.99)
    time.sleep(1)
```

**Step 3: Consumer - ë°ì´í„° ì²˜ë¦¬ (Lambda)**

```python
# lambda_kinesis_consumer.py
import json
import base64

def lambda_handler(event, context):
    for record in event['Records']:
        # Kinesis ë°ì´í„°ëŠ” base64 ì¸ì½”ë”©ë˜ì–´ ìˆìŒ
        payload = base64.b64decode(record['kinesis']['data'])
        data = json.loads(payload)

        # ë°ì´í„° ì²˜ë¦¬
        print(f"Processing: {data}")

        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ì˜ˆ: DynamoDB ì €ì¥)
        if data['action'] == 'purchase':
            save_to_dynamodb(data)

    return {
        'statusCode': 200,
        'body': json.dumps('Successfully processed records')
    }

def save_to_dynamodb(data):
    import boto3
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('Purchases')

    table.put_item(
        Item={
            'user_id': data['user_id'],
            'timestamp': data['timestamp'],
            'value': str(data['value'])
        }
    )
```

**Step 4: Lambdaë¥¼ Kinesis íŠ¸ë¦¬ê±°ë¡œ ì—°ê²°**

```bash
# Lambda í•¨ìˆ˜ì— Kinesis íŠ¸ë¦¬ê±° ì¶”ê°€
aws lambda create-event-source-mapping \
    --function-name KinesisConsumer \
    --event-source-arn arn:aws:kinesis:us-east-1:ACCOUNT:stream/my-data-stream \
    --batch-size 100 \
    --starting-position LATEST
```

---

### ğŸ¤– AWS SageMaker - ê¸°ê³„í•™ìŠµ í”Œë«í¼

**ê°œë…:**
AWS SageMakerëŠ” **ê¸°ê³„í•™ìŠµ ëª¨ë¸ì„ ë¹Œë“œ, í•™ìŠµ, ë°°í¬**í•˜ëŠ” ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ê¸°ê³„í•™ìŠµ ìœ í˜•:**

```mermaid
graph TD
    A[ê¸°ê³„í•™ìŠµ] --> B[ì§€ë„í•™ìŠµ<br>Supervised]
    A --> C[ë¹„ì§€ë„í•™ìŠµ<br>Unsupervised]
    A --> D[ê°•í™”í•™ìŠµ<br>Reinforcement]

    B --> B1[ë¶„ë¥˜ Classification]
    B --> B2[íšŒê·€ Regression]

    C --> C1[í´ëŸ¬ìŠ¤í„°ë§]
    C --> C2[ì°¨ì› ì¶•ì†Œ]

    D --> D1[ë³´ìƒ ê¸°ë°˜ í•™ìŠµ]

    B1 -.-> B1E[ì´ë©”ì¼ ìŠ¤íŒ¸ í•„í„°]
    B2 -.-> B2E[ì£¼íƒ ê°€ê²© ì˜ˆì¸¡]
    C1 -.-> C1E[ê³ ê° ì„¸ë¶„í™”]
    D1 -.-> D1E[AlphaGo]
```

**SageMaker ì‘ì—… íë¦„:**

```mermaid
flowchart TD
    A[ë°ì´í„° ìˆ˜ì§‘] --> B[S3 ì €ì¥]
    B --> C[SageMaker Processing<br>ë°ì´í„° ì „ì²˜ë¦¬]
    C --> D[SageMaker Training<br>ëª¨ë¸ í•™ìŠµ]
    D --> E{ëª¨ë¸ í‰ê°€}
    E -->|ì„±ëŠ¥ ë¶€ì¡±| F[í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹]
    F --> D
    E -->|ì„±ëŠ¥ ì¶©ë¶„| G[SageMaker Model Registry<br>ëª¨ë¸ ë“±ë¡]
    G --> H[SageMaker Endpoint<br>ë°°í¬]
    H --> I[ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì„œë¹„ìŠ¤]

    style C fill:#e1f5ff
    style D fill:#e8f5e9
    style H fill:#fce4ec
```

**SageMaker ì£¼ìš” ê¸°ëŠ¥:**

1. **SageMaker Studio** - í†µí•© ê°œë°œ í™˜ê²½
2. **SageMaker Autopilot** - AutoML (ìë™ ëª¨ë¸ ìƒì„±)
3. **SageMaker Training** - ë¶„ì‚° í•™ìŠµ
4. **SageMaker Hyperparameter Tuning** - ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
5. **SageMaker Model Monitor** - ëª¨ë¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
6. **SageMaker Feature Store** - íŠ¹ì„± ì €ì¥ì†Œ

ğŸ’¡ **ì¤‘ìš”!**: SageMakerëŠ” ë°ì´í„° ê³¼í•™ìì™€ ML ì—”ì§€ë‹ˆì–´ê°€ ML ì¸í”„ë¼ ê´€ë¦¬ ì—†ì´ ëª¨ë¸ ê°œë°œì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

---

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë¶„ì„ ì„œë¹„ìŠ¤)

**Amazon Athena:**
- [ ] S3 ë°ì´í„°ë¥¼ SQLë¡œ ì¿¼ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] ì™¸ë¶€ í…Œì´ë¸”ì„ ìƒì„±í•˜ê³  íŒŒí‹°ì…”ë‹í•  ìˆ˜ ìˆë‹¤
- [ ] ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ Parquet í˜•ì‹ì„ í™œìš©í•œë‹¤

**AWS Glue:**
- [ ] Glue Crawlerë¡œ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ ê²€ìƒ‰í•  ìˆ˜ ìˆë‹¤
- [ ] Glue ETL Jobìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•  ìˆ˜ ìˆë‹¤
- [ ] PySparkë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì´í•´í•œë‹¤

**Amazon OpenSearch:**
- [ ] OpenSearch ë„ë©”ì¸ì„ ìƒì„±í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•˜ê³  ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] ELK ìŠ¤íƒì˜ ê°œë…ì„ ì´í•´í•œë‹¤

**Amazon EMR:**
- [ ] EMR í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] Spark ì‘ì—…ì„ ì œì¶œí•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] Hadoop ìƒíƒœê³„ì˜ ë„êµ¬ë“¤ì„ ì´í•´í•œë‹¤

**Amazon Kinesis:**
- [ ] Kinesis Data Streamsì™€ Firehoseì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] Lambdaì™€ Kinesisë¥¼ í†µí•©í•  ìˆ˜ ìˆë‹¤

**AWS SageMaker:**
- [ ] ê¸°ê³„í•™ìŠµì˜ 3ê°€ì§€ ìœ í˜•ì„ ì´í•´í•œë‹¤
- [ ] SageMakerì˜ ì‘ì—… íë¦„ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] ML íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ì´í•´í•œë‹¤


## ğŸ› ï¸ ì‹œìŠ¤í…œ ê´€ë¦¬ ì„œë¹„ìŠ¤

### â˜ï¸ AWS CloudFormation - Infrastructure as Code (IaC)

**ê°œë…:**
AWS ë¦¬ì†ŒìŠ¤ë¥¼ **ì½”ë“œ(í…œí”Œë¦¿)**ë¡œ ì •ì˜í•˜ì—¬ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” IaC ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ë°˜ë³µ ê°€ëŠ¥** | ê°™ì€ í…œí”Œë¦¿ìœ¼ë¡œ ë™ì¼í•œ í™˜ê²½ ì¬ìƒì„± |
| **ë²„ì „ ê´€ë¦¬** | Git ë“±ìœ¼ë¡œ ì¸í”„ë¼ ë²„ì „ ê´€ë¦¬ |
| **ë“œë¦¬í”„íŠ¸ ê°ì§€** | í…œí”Œë¦¿ê³¼ ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì°¨ì´ ê°ì§€ |
| **ë¡¤ë°±** | ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°± |

**í…œí”Œë¦¿ êµ¬ì¡° (YAML):**

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'VPC and EC2 Instance'

Parameters:
  VpcCIDR:
    Type: String
    Default: '10.20.0.0/16'

Resources:
  MyVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCIDR
      Tags:
        - Key: Name
          Value: Lab-VPC

  MyEC2:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-0c55b159cbfafe1f0
      InstanceType: t3.micro
      SubnetId: !Ref PublicSubnet
```

**ì‹¤ìŠµì—ì„œ ë°°ìš´ ë‚´ìš©:**
- CloudFormation ìŠ¤íƒ ìƒì„±
- í…œí”Œë¦¿ìœ¼ë¡œ VPC, ì„œë¸Œë„·, EC2 ìë™ ìƒì„±
- ìŠ¤íƒ ì‚­ì œ ì‹œ ë¦¬ì†ŒìŠ¤ ì¼ê´„ ì‚­ì œ

---

### ğŸ“Š Amazon CloudWatch - ëª¨ë‹ˆí„°ë§ ë° ê²½ë³´

**ì£¼ìš” ê¸°ëŠ¥:**

1. **ë©”íŠ¸ë¦­ (Metrics)** - ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
2. **ê²½ë³´ (Alarms)** - ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
3. **ëŒ€ì‹œë³´ë“œ (Dashboards)** - ì‹œê°í™”
4. **Logs** - ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„

**ì‹¤ìŠµ: CloudWatch ê²½ë³´ ìƒì„±**

```bash
# SNS í† í”½ ìƒì„± (ì•Œë¦¼ìš©)
aws sns create-topic --name alarm-notifications

# ì´ë©”ì¼ êµ¬ë…
aws sns subscribe \
    --topic-arn arn:aws:sns:region:account:alarm-notifications \
    --protocol email \
    --notification-endpoint your-email@example.com

# CloudWatch ê²½ë³´ ìƒì„± (CPU 60% ì´ˆê³¼)
aws cloudwatch put-metric-alarm \
    --alarm-name HighCPUAlarm \
    --alarm-description "CPU exceeds 60%" \
    --metric-name CPUUtilization \
    --namespace AWS/EC2 \
    --statistic Average \
    --period 300 \
    --threshold 60 \
    --comparison-operator GreaterThanThreshold \
    --dimensions Name=InstanceId,Value=i-1234567890abcdef0 \
    --evaluation-periods 1 \
    --alarm-actions arn:aws:sns:region:account:alarm-notifications
```

---

### ğŸ”§ AWS Systems Manager

**ì£¼ìš” ì„œë¹„ìŠ¤:**

1. **Session Manager** - í‚¤ ì—†ì´ EC2 ì ‘ì†
2. **Parameter Store** - ì„¤ì • ì •ë³´ ì•ˆì „ ì €ì¥
3. **Patch Manager** - ìë™ íŒ¨ì¹˜ ê´€ë¦¬
4. **Run Command** - ì›ê²© ëª…ë ¹ ì‹¤í–‰
5. **State Manager** - ìƒíƒœ ìœ ì§€ ê´€ë¦¬

---

### ğŸ”” Amazon EventBridge - ì´ë²¤íŠ¸ ê¸°ë°˜ ìë™í™”

**ì‹¤ìŠµ: EC2 ì¤‘ì§€ ì‹œ ì´ë©”ì¼ ì•Œë¦¼**

```bash
# EventBridge ê·œì¹™ ìƒì„±
aws events put-rule \
    --name EC2StateChange \
    --event-pattern '{
        "source": ["aws.ec2"],
        "detail-type": ["EC2 Instance State-change Notification"],
        "detail": {
            "state": ["stopped", "terminated"],
            "instance-id": ["i-1234567890abcdef0"]
        }
    }'

# SNSë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì—°ê²°
aws events put-targets \
    --rule EC2StateChange \
    --targets "Id"="1","Arn"="arn:aws:sns:region:account:alarm-notifications"
```

---

## ğŸ¯ ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸: ì›¹ ì„œë¹„ìŠ¤ ì¸í”„ë¼ êµ¬ì¶•

### ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ:** ê°€ìš©ì„±, í™•ì¥ì„±, ë³´ì•ˆì„±ì„ ê°–ì¶˜ 3-Tier ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸í”„ë¼ êµ¬ì¶•

**ì•„í‚¤í…ì²˜:**

```mermaid
graph TD
    subgraph Public[Public Subnet]
        ALB[Application Load Balancer]
        NAT1[NAT Gateway AZ-A]
        NAT2[NAT Gateway AZ-C]
    end

    subgraph Private1[App Subnet AZ-A]
        APP1[App Server 1<br>Auto Scaling]
    end

    subgraph Private2[App Subnet AZ-C]
        APP2[App Server 2<br>Auto Scaling]
    end

    subgraph DB1[DB Subnet AZ-A]
        RDS1[Aurora Writer]
    end

    subgraph DB2[DB Subnet AZ-C]
        RDS2[Aurora Reader]
    end

    Internet[Internet] --> ALB
    ALB --> APP1
    ALB --> APP2

    APP1 --> EFS[EFS<br>Shared Storage]
    APP2 --> EFS

    APP1 --> RDS1
    APP2 --> RDS1
    RDS1 -.ë³µì œ.-> RDS2

    APP1 -.-> NAT1
    APP2 -.-> NAT2

    style ALB fill:#ff9999
    style APP1 fill:#99ccff
    style APP2 fill:#99ccff
    style RDS1 fill:#99ff99
    style EFS fill:#ffff99
```

### ğŸ”§ êµ¬ì¶• ë‹¨ê³„

**1ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ êµ¬ì„±**
- VPC ìƒì„± (ë©€í‹° AZ)
- Public/Private ì„œë¸Œë„· ë¶„ë¦¬
- NAT Gateway ì´ì¤‘í™”

**2ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤**
- Aurora MySQL í´ëŸ¬ìŠ¤í„° (Multi-AZ)
- Parameter Storeì— ì ‘ì† ì •ë³´ ì €ì¥

**3ë‹¨ê³„: ê³µìœ  ìŠ¤í† ë¦¬ì§€**
- EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„±
- App ì„œë²„ì—ì„œ ë§ˆìš´íŠ¸

**4ë‹¨ê³„: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë²„**
- Launch Template ìƒì„±
- Auto Scaling Group êµ¬ì„±
- User Dataë¡œ ìë™ êµ¬ì„±

**5ë‹¨ê³„: ë¡œë“œ ë°¸ëŸ°ì„œ**
- ALB ìƒì„±
- Target Group (í¬íŠ¸ 5000)
- Health Check ì„¤ì •

### ğŸ“ Python Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜ˆì œ

```python
# app.py
from flask import Flask, render_template
import boto3
import pymysql

app = Flask(__name__)

# Parameter Storeì—ì„œ DB ì •ë³´ ì½ê¸°
ssm = boto3.client('ssm', region_name='us-east-1')

def get_parameter(name):
    response = ssm.get_parameter(Name=name, WithDecryption=True)
    return response['Parameter']['Value']

DB_HOST = get_parameter('/myapp/db/host')
DB_USER = get_parameter('/myapp/db/user')
DB_PASS = get_parameter('/myapp/db/password')

@app.route('/')
def index():
    conn = pymysql.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASS,
        database='mydb'
    )
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM users")
    user_count = cursor.fetchone()[0]
    conn.close()

    return f"<h1>Hello from Auto Scaling!</h1><p>Total users: {user_count}</p>"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## ğŸ“Š ìµœì¢… í†µê³„ ë° ìš”ì•½

### âœ… í•™ìŠµí•œ ì£¼ìš” ì„œë¹„ìŠ¤

**ë°ì´í„° ë¶„ì„:**
- Amazon Athena (S3 SQL ì¿¼ë¦¬)
- AWS Glue (ETL)
- Amazon OpenSearch (ë¡œê·¸ ë¶„ì„)
- Amazon EMR (ë¹…ë°ì´í„° ì²˜ë¦¬)
- Amazon Kinesis (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
- AWS SageMaker (ê¸°ê³„í•™ìŠµ)

**ì‹œìŠ¤í…œ ê´€ë¦¬:**
- AWS CloudFormation (IaC)
- Amazon CloudWatch (ëª¨ë‹ˆí„°ë§)
- AWS Systems Manager (ìš´ì˜ ê´€ë¦¬)
- Amazon EventBridge (ì´ë²¤íŠ¸ ê¸°ë°˜ ìë™í™”)

**ê°œë°œ ì§€ì›:**
- AWS CodeCommit (Git)
- AWS CodeBuild (ë¹Œë“œ)
- AWS CodeDeploy (ë°°í¬)
- AWS CodePipeline (CI/CD)

### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **ë°ì´í„° ë ˆì´í¬ vs ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤**
   - ë ˆì´í¬: ì›ì‹œ ë°ì´í„° ì €ì¥ (S3)
   - ì›¨ì–´í•˜ìš°ìŠ¤: ì •ì œëœ ë°ì´í„° ì¿¼ë¦¬ (Redshift)

2. **ì‹¤ì‹œê°„ vs ë°°ì¹˜ ë¶„ì„**
   - ì‹¤ì‹œê°„: Kinesis + Lambda
   - ë°°ì¹˜: S3 + Glue + Athena

3. **IaCì˜ ì¤‘ìš”ì„±**
   - ì¬í˜„ ê°€ëŠ¥í•œ ì¸í”„ë¼
   - ë²„ì „ ê´€ë¦¬
   - ìë™í™”

4. **ëª¨ë‹ˆí„°ë§ê³¼ ê²½ë³´**
   - CloudWatchë¡œ ì‚¬ì „ ì¥ì•  ê°ì§€
   - EventBridgeë¡œ ìë™ ëŒ€ì‘

### ğŸ“ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì „ì²´)

**ë°ì´í„° ë¶„ì„:**
- [ ] ë°°ì¹˜ ë¶„ì„ê³¼ ì‹¤ì‹œê°„ ë¶„ì„ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] S3ë¥¼ ë°ì´í„° ë ˆì´í¬ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤
- [ ] Athenaë¡œ S3 ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] Glueë¡œ ETL íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- [ ] Kinesisë¡œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤

**ì‹œìŠ¤í…œ ê´€ë¦¬:**
- [ ] CloudFormation í…œí”Œë¦¿ì„ ì‘ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] CloudWatch ê²½ë³´ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤
- [ ] Systems Managerë¡œ EC2ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] EventBridgeë¡œ ì´ë²¤íŠ¸ ê¸°ë°˜ ìë™í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

**í”„ë¡œì íŠ¸:**
- [ ] Multi-AZ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤
- [ ] Auto Scalingê³¼ ELBë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] EFSë¡œ ê³µìœ  ìŠ¤í† ë¦¬ì§€ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] Parameter Storeë¡œ ì„¤ì • ì •ë³´ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤

---

## ğŸ‰ 7ì¼ê°„ì˜ AWS í´ë¼ìš°ë“œ êµìœ¡ ì™„ë£Œ!

**í•™ìŠµí•œ ë‚´ìš© ìš”ì•½:**

| ì¼ì°¨ | ì£¼ì œ |
|------|------|
| 1ì¼ì°¨ | AWS ê¸°ì´ˆ, IAM, VPC |
| 2ì¼ì°¨ | EC2, EBS, EFS |
| 3ì¼ì°¨ | ELB, Auto Scaling |
| 4ì¼ì°¨ | RDS, DynamoDB |
| 5ì¼ì°¨ | S3, Lambda, ì„œë²„ë¦¬ìŠ¤ |
| 6ì¼ì°¨ | Redshift, DMS, ë³´ì•ˆ ì„œë¹„ìŠ¤ |
| **7ì¼ì°¨** | **ë°ì´í„° ë¶„ì„, ì‹œìŠ¤í…œ ê´€ë¦¬** |

**ë‹¤ìŒ ë‹¨ê³„:**
- AWS ìê²©ì¦ ì¤€ë¹„ (Solutions Architect Associate)
- ì‹¤ì œ í”„ë¡œì íŠ¸ì— AWS ì„œë¹„ìŠ¤ ì ìš©
- ë¹„ìš© ìµœì í™” ë° ì„±ëŠ¥ íŠœë‹ í•™ìŠµ
- ê³ ê¸‰ ì„œë¹„ìŠ¤ (EKS, ECS, Fargate) í•™ìŠµ

---

**ê°•ì˜ ë…¸íŠ¸ ì‘ì„± ì™„ë£Œ!** ğŸ“âœ¨

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 11ì¼
**ì´ ì¤„ ìˆ˜**: 2000+ ì¤„
**ì‘ì„± ë°©ì‹**: STT ê¸°ë°˜ ìƒì„¸ ì •ë¦¬
**íŠ¹ì§•**: 
- ê·¹ë„ë¡œ ìƒì„¸í•œ ì„¤ëª…
- ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ ì˜ˆì œ
- Mermaid ë‹¤ì´ì–´ê·¸ë¨
- ì‹¤ìŠµ ê°€ì´ë“œ í¬í•¨
- ë³´ì•ˆ ê³ ë ¤ì‚¬í•­ í¬í•¨

