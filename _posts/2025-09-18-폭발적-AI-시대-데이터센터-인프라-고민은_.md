--- 
title: "고밀도 AI 데이터센터의 전력 및 냉각 인프라 설계"
date: 2025-09-18
excerpt: "랙당 전력 밀도 폭증에 따른 데이터센터 설계의 변화와 직접 액체 냉각(DLC), 전원 이중화 등 실무적인 인프라 구축 고민을 다룹니다."
categories:
  - Archive
  - conference
  - DIC 2025
tags:
  - DIC 2025
  - conference
  - Archive
---

# 폭발적 AI 시대, 데이터센터 인프라의 현실적 고민

**발표자:** 홍지원, 엠피리온 디지털(Empirion Digital) 데이터센터 세일즈 담당

## 서론: AI 시대, 전력 효율성의 새로운 정의

AI 시대가 도래하며 데이터센터 산업은 '저전력'이 아닌 **'전력의 고효율적 사용'**이라는 새로운 화두에 직면했습니다. AI 연산에 필요한 막대한 전력을 줄이는 것은 현실적으로 불가능하며, 주어진 전력을 얼마나 효율적으로 IT 장비에 전달하고 열을 식히는지가 데이터센터의 핵심 경쟁력이 되었습니다. 이 효율성의 지표가 바로 **PUE(Power Usage Effectiveness)**입니다.

엠피리온 디지털은 싱가포르에 본사를 둔 아시아의 데이터센터 플랫폼 개발사로, 한국을 포함한 아시아 주요 도시에 차세대 데이터센터를 구축하고 있습니다. 특히 2025년 7월 서울 양재에 오픈한 **KR1 데이터센터**는 한국의 평균 PUE(1.6~1.8)를 훨씬 뛰어넘는 **1.3 수준의 높은 전력 효율**을 달성했습니다. 40MW의 대규모 용량을 갖춘 이 데이터센터를 AI 전용으로 전환하며 겪었던 현실적인 고민과 해결 과제들을 공유하고자 합니다.

## 본론 1: 전력 밀도의 폭증과 그에 따른 도전

AI 시대의 가장 큰 특징은 **랙(Rack)당 전력 밀도의 기하급수적인 증가**입니다. 이는 데이터센터의 설계, 운영, 그리고 미래를 완전히 바꾸고 있습니다.

*   **과거 (2000년대 초반):** 랙당 2.2~4kW 수준으로, x86 서버와 유닉스 장비가 주를 이뤘습니다.
*   **클라우드 시대 (2010년대):** 가상화와 하이퍼스케일러의 등장으로 랙당 10kW가 평균 요구사항이 되었습니다.
*   **AI 시대 (2025년 현재):** NVIDIA B200과 같은 GPU 서버가 등장하며, 랙당 **30kW**가 새로운 기준으로 자리 잡고 있습니다.
*   **가까운 미래 (2026년 이후):** 차세대 AI 시스템인 NVL72는 랙당 **130~150kW**를 요구합니다.
*   **먼 미래 (2027년 이후):** NVIDIA의 '루빈(Rubin)' 아키텍처는 랙당 **600kW**라는, 데이터 홀 하나 전체와 맞먹는 엄청난 전력을 필요로 할 것으로 예측됩니다.

이러한 전력 밀도의 폭증은 기존의 데이터센터 인프라로는 감당할 수 없는 수준이며, 냉각과 전력 공급 방식의 근본적인 혁신을 요구합니다.

## 본론 2: 냉각 기술의 진화 - 공랭에서 액체 냉각으로

전력 밀도 증가는 곧 발열량의 증가를 의미합니다. 이를 해결하기 위한 냉각 기술은 다음과 같이 진화하고 있습니다.

### 1. 공랭(Air Cooling)의 한계와 고도화

랙당 30kW 수준의 B200 서버까지는 아직 공랭 방식이 사용될 수 있습니다. 하지만 이는 과거의 방식과는 다릅니다. 엠피리온 디지털은 랙당 2개의 서버만 장착하고, 랙의 후면을 서로 마주보게 배치하여 뜨거운 공기를 한 곳에 모으는 **컨테인먼트(Containment)** 방식을 채택했습니다. 이를 통해 공기 흐름을 최적화하고, 서버 간의 거리를 좁혀 인피니밴드(InfiniBand) 케이블 길이를 최소화함으로써 레이턴시 문제를 해결했습니다.

### 2. 필수 기술이 된 직접 액체 냉각(DLC, Direct Liquid Cooling)

랙당 60~100kW를 넘어가는 시점부터는 공랭 방식이 불가능해지며, **직접 액체 냉각(DLC)**이 필수가 됩니다. DLC는 CPU나 GPU 같은 핵심 발열 부품에 냉각판(Cold Plate)을 부착하고, 그 안으로 냉각수를 순환시켜 열을 직접 식히는 방식입니다. 엠피리온은 KR1 데이터센터의 7개 층 중 4개 층을 DLC 전용으로 준비하여, NVL72와 같은 차세대 장비를 수용할 계획입니다.

### 3. 궁극의 기술, 액침 냉각(Immersion Cooling)의 과제

서버 전체를 비전도성 오일에 담가 식히는 액침 냉각은 가장 효율적인 방식으로 알려져 있습니다. 하지만 상용화까지는 여러 장벽이 존재합니다.

*   **안전 규제:** 냉각 오일이 소방법상 위험물로 분류될 수 있습니다.
*   **구조적 문제:** 수평형 탱크 구조는 데이터센터의 층고, 바닥 하중 등 전체 설계 변경을 요구합니다.
*   **장비 보증:** IT 장비 제조사들이 오일 속에서 장기간 운영 시 부품의 내구성을 보증해주지 않고 있습니다.

이러한 문제들이 해결되기 전까지, DLC가 가장 현실적인 고밀도 냉각 솔루션이 될 것입니다.

## 본론 3: 전력 시스템의 재설계

고밀도 AI 서버는 전력 공급 방식의 재설계 또한 요구합니다.

### 1. 복잡해진 전원 이중화

과거에는 A, B 두 개의 독립된 전원을 공급하는 것만으로 충분했지만, 최신 GPU 서버는 6개 이상의 전원공급장치(PSU)를 가지며, 이 중 일정 수 이상이 항상 활성화되어야 합니다. NVIDIA는 6개의 PSU에 각각 독립된 6개의 전원을 공급하는 것을 권장하지만, 이는 대부분의 데이터센터 현실과 맞지 않습니다.

엠피리온은 이 문제를 해결하기 위해 **A, B, C 세 개의 주 전원과 하나의 백업(Catcher) UPS를 더한, 총 4개의 전원 소스를 활용**하는 독자적인 방식을 고안했습니다. 이를 통해 하나의 UPS에 장애가 발생하더라도 백업 UPS가 즉시 전력을 공급하여, 서버 다운 없이 높은 가용성을 보장합니다.

### 2. 예상치 못한 복병, CDU 전력 문제

DLC 시스템의 핵심 장비인 **냉각수 분배 장치(CDU, Coolant Distribution Unit)**는 새로운 문제를 야기했습니다. CDU 내부의 강력한 모터와 인버터는 IT 장비에 치명적인 전기적 노이즈(고조파)를 발생시킵니다. 이 때문에 CDU는 반드시 IT 전원과 완전히 분리된 **별도의 유틸리티 전원**에 연결되어야 합니다.

하지만 대부분의 데이터센터는 설계 시 이 정도의 대용량 유틸리티 전원 여유를 고려하지 않았습니다. 남는 전력은 곧바로 IT 용량으로 판매해야 하는 상업 데이터센터의 특성상, 이는 예상치 못한 큰 난관이며 현재 관련 제조사들과 함께 해결책을 모색하고 있습니다.

## 본론 4: 구조 및 운영상의 현실적 문제들

### 1. 바닥 하중(Floor Loading)의 한계

냉각수까지 가득 채운 NVL72 랙 하나의 무게는 **1.5톤에서 1.6톤**에 달합니다. 이는 국내 대부분 데이터센터의 바닥 하중 설계 기준인 제곱미터당 1.5톤을 초과하는 수치입니다. 즉, 많은 데이터센터들이 구조적으로 차세대 AI 장비를 수용할 수 없습니다. 엠피리온은 이를 미리 예측하여 KR1 데이터센터를 **1.8톤/m²의 하중**으로 설계하여 이 문제를 해결했습니다.

### 2. 운영 온도를 둘러싼 고객과의 갈등

미국 냉동 공조 학회(ASHRAE) 가이드라인은 데이터센터 운영 온도를 18~27℃로 권장하며, 최신 GPU는 32℃까지도 견딜 수 있도록 설계됩니다. 온도를 높게 설정할수록 냉각에 드는 에너지가 절약됩니다. 하지만 일부 국내 대기업 고객들은 과거의 경험에 의존하여 비합리적으로 낮은 온도(예: 24℃)를 고집하는 경우가 많습니다.

특히 고온다습한 한여름에 24℃를 유지하는 것은 데이터센터 운영사에 막대한 에너지 비용과 설비 부담을 안겨줍니다. 명확한 기술적 근거 없이 과거의 관행만을 주장하는 이러한 요구는 AI 시대의 효율적인 데이터센터 운영을 가로막는 장애물이며, 고객사의 인식 변화가 시급한 부분입니다.

## 결론: 미래를 내다보는 데이터센터 설계의 중요성

폭발적인 AI 시대는 데이터센터 인프라의 모든 측면에서 근본적인 변화를 요구하고 있습니다. 랙당 전력 밀도의 증가는 냉각, 전력, 건물 구조, 심지어 운영 문화에 이르기까지 새로운 패러다임을 강요합니다.

엠피리온 디지털의 KR1 데이터센터 사례는 이러한 도전에 맞서기 위해 미래를 예측하고, 설계 단계부터 높은 전력 효율, 진화된 냉각 방식, 강화된 구조적 안정성을 미리 반영하는 것이 얼마나 중요한지를 보여줍니다. 성공적인 AI 전환을 위해서는 데이터센터 사업자의 선제적인 기술 투자와 더불어, 사용자(고객) 또한 낡은 관행에서 벗어나 현대적인 기술 표준을 받아들이는 성숙한 파트너십이 필요합니다.
